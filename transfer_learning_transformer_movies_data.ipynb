{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "II784z1uYNdS",
        "wryP8k9KXdPm",
        "s2so4O8CYpEJ",
        "k-V9JzqnxeHD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moharamfatema/graduation-project/blob/main/transfer_learning_transformer_movies_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1r_ElyehZwlWC7je1ZCsrauqKAObqnjNN?usp=sharing)\n",
        "\n",
        "# Violence Detection In Movie Scenes"
      ],
      "metadata": {
        "id": "OFgTD_PAXmUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the runtime"
      ],
      "metadata": {
        "id": "II784z1uYNdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-convert"
      ],
      "metadata": {
        "id": "YzX5K-pfW0DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0cc1425-6786-4434-9714-10d8fb4a1fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colab-convert\n",
            "  Downloading colab-convert-2.0.5.tar.gz (18 kB)\n",
            "Collecting json5\n",
            "  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: colab-convert\n",
            "  Building wheel for colab-convert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colab-convert: filename=colab_convert-2.0.5-py3-none-any.whl size=19224 sha256=6716d0924e59e731a6de2da4ff597d4d0eba818c33be6d5bac8a386c08ab2edd\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/f2/9d/ee946c8db91ba06736f0fc6d0391421f891d5e19785ce707f6\n",
            "Successfully built colab-convert\n",
            "Installing collected packages: json5, colab-convert\n",
            "Successfully installed colab-convert-2.0.5 json5-0.9.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "id": "4hg5Eb1iq6ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b9866a-24ad-44b5-eadd-02efdeaf8df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show colab-convert\n",
        "print()\n",
        "!pip show numpy\n",
        "print()\n",
        "!pip show tensorflow\n",
        "print()\n",
        "!pip show opencv-python\n",
        "print()\n",
        "!pip show pyyaml\n",
        "print()\n",
        "!pip show h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HNxQirlYbcB",
        "outputId": "30ce07d3-2da1-4317-9ac2-3287c1f9d5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: colab-convert\n",
            "Version: 2.0.5\n",
            "Summary: Convert .py files runnable in VSCode/Python or Atom/Hydrogen to jupyter/colab .ipynb notebooks and vice versa\n",
            "Home-page: https://github.com/MSFTserver/colab-convert\n",
            "Author: HostsServer\n",
            "Author-email: msftserver@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: json5\n",
            "Required-by: \n",
            "\n",
            "Name: numpy\n",
            "Version: 1.21.6\n",
            "Summary: NumPy is the fundamental package for array computing with Python.\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: None\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: yellowbrick, xgboost, xarray, xarray-einstats, wordcloud, torchvision, torchtext, tifffile, thinc, tensorflow, tensorflow-probability, tensorflow-hub, tensorflow-datasets, tensorboard, tables, statsmodels, spacy, sklearn-pandas, seaborn, scs, scipy, scikit-learn, scikit-image, resampy, qudida, qdldl, PyWavelets, python-louvain, pystan, pysndfile, pymc, pyerfa, pyemd, pycocotools, pyarrow, prophet, plotnine, patsy, pandas, osqp, opt-einsum, opencv-python, opencv-python-headless, opencv-contrib-python, numexpr, numba, nibabel, netCDF4, moviepy, mlxtend, mizani, missingno, matplotlib, matplotlib-venn, lightgbm, librosa, Keras-Preprocessing, kapre, jpeg4py, jaxlib, jax, imgaug, imbalanced-learn, imageio, hyperopt, httpstan, holoviews, h5py, gym, gensim, folium, fix-yahoo-finance, fastdtw, fa2, ecos, datascience, daft, cvxpy, cufflinks, cmdstanpy, cftime, bokeh, blis, autograd, atari-py, astropy, arviz, altair, albumentations, aesara, aeppl\n",
            "\n",
            "Name: tensorflow\n",
            "Version: 2.8.2+zzzcolab20220719082949\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: tensorboard, typing-extensions, tensorflow-estimator, gast, google-pasta, numpy, grpcio, h5py, flatbuffers, termcolor, keras-preprocessing, wrapt, protobuf, six, libclang, astunparse, absl-py, tensorflow-io-gcs-filesystem, opt-einsum, keras, setuptools\n",
            "Required-by: kapre\n",
            "\n",
            "Name: opencv-python\n",
            "Version: 4.6.0.66\n",
            "Summary: Wrapper package for OpenCV python bindings.\n",
            "Home-page: https://github.com/skvark/opencv-python\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy\n",
            "Required-by: imgaug, dopamine-rl\n",
            "\n",
            "Name: PyYAML\n",
            "Version: 6.0\n",
            "Summary: YAML parser and emitter for Python\n",
            "Home-page: https://pyyaml.org/\n",
            "Author: Kirill Simonov\n",
            "Author-email: xi@resolvent.net\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: tensorflow-docs, PyDrive, fastai, distributed, dask, bokeh, albumentations\n",
            "\n",
            "Name: h5py\n",
            "Version: 3.1.0\n",
            "Summary: Read and write HDF5 files from Python\n",
            "Home-page: http://www.h5py.org\n",
            "Author: Andrew Collette\n",
            "Author-email: andrew.collette@gmail.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy, cached-property\n",
            "Required-by: tensorflow, keras-vis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "wryP8k9KXdPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y09Bw_IGCtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da17297f-ec46-42ff-eb30-bbbfa9456351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/.shortcut-targets-by-id/1lB9yrEkShjexDmBZKReu7bjw8lfmUnmH/automatic-detection-of-violence-in-video-scenes\n",
            "cc-outputs.log    \u001b[0m\u001b[01;34m.git\u001b[0m/               MViTv2_L_40x3_k400_f306903192.pyth\n",
            "\u001b[01;34mCombined\u001b[0m/         .gitignore          new_repo.sh\n",
            "config_user.sh    git_interactive.py  requirements.txt\n",
            "convert.txt       \u001b[01;36mHockey\u001b[0m@             svm_model.pkl\n",
            "\u001b[01;36mCrowd2\u001b[0m@           info.txt            sync.sh\n",
            "forest_model.pkl  \u001b[01;34mMovies\u001b[0m/             \u001b[01;34mtransformers\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%cd '/content/drive/MyDrive/automatic-detection-of-violence-in-video-scenes'\n",
        "%ls -a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "s2so4O8CYpEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow      import keras\n",
        "from keras           import layers as tfl\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.backend   import clear_session\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2 \n",
        "import datetime\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "tf.test.gpu_device_name()\n",
        "# Standard output is '/device:GPU:0'"
      ],
      "metadata": {
        "id": "lt1G6WwGYv-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01977351-ab28-46db-effb-217dc114c291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "YhHJ07ZWZCzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Constants"
      ],
      "metadata": {
        "id": "k-V9JzqnxeHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define input paths\n",
        "\n",
        "ROOT_PATH = '/content/drive/MyDrive/automatic-detection-of-violence-in-video-scenes'\n",
        "MOVIES_PATH = ROOT_PATH + '/Movies/Movies Fight Dataset'\n",
        "\n",
        "VIOLENCE_PATH = MOVIES_PATH + '/Violence'\n",
        "NORMAL_PATH = MOVIES_PATH + '/Normal'\n",
        "\n",
        "print(os.listdir(NORMAL_PATH)[-1])\n",
        "print(os.listdir(VIOLENCE_PATH)[-1])\n",
        "\n",
        "# define output paths\n",
        "\n",
        "OUT_PATH = ROOT_PATH + '/transformers/preprocessed_data'\n"
      ],
      "metadata": {
        "id": "tKkvBlmCZF-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb8d1dc-94ec-40e2-ab2d-a2c39e2f92c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60.mpg\n",
            "newfi67.avi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process raw data"
      ],
      "metadata": {
        "id": "fL4v8Detxl5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 50\n",
        "\n",
        "def load_video(path, max_frames=0, dimension=IMG_SIZE):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        i = -1\n",
        "        while True:\n",
        "            i += 1\n",
        "            if i % 2 != 0:\n",
        "                continue\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, (dimension, dimension), interpolation = cv2.INTER_AREA)\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n"
      ],
      "metadata": {
        "id": "bL0emhASlS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_extractor(dimension=IMG_SIZE):\n",
        "    feature_extractor = keras.applications.DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(dimension, dimension,3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((dimension, dimension,3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")"
      ],
      "metadata": {
        "id": "iaUbucRCrujy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = build_feature_extractor()"
      ],
      "metadata": {
        "id": "f_Q9BTmym53b",
        "outputId": "3dee514c-6bc5-487f-9a12-6bbf9d3032eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "29097984/29084464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 50\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir=MOVIES_PATH):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"label\"].values\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir,NORMAL_PATH if labels[idx]==0 else VIOLENCE_PATH, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels"
      ],
      "metadata": {
        "id": "r395IB8FnGSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_paths(normal, violence):\n",
        "  subset = min(len(normal),len(violence))\n",
        "\n",
        "  y = np.concatenate([np.zeros(subset), np.ones(subset)])\n",
        "  x = np.concatenate([normal[:subset], violence[:subset]], axis = 0)\n",
        "\n",
        "  print((x.shape, y.shape))\n",
        "\n",
        "  #shuffle\n",
        "  per = np.random.permutation(len(x))\n",
        "\n",
        "  x = x[per]\n",
        "  y = y[per]\n",
        "  y = y.astype(int)\n",
        "\n",
        "  y = np.expand_dims(y, -1)\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "nFlQIM1Hq8e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_datasets(\n",
        "    save_path: str\n",
        "  ) -> np.ndarray:\n",
        "  \n",
        "  normal_random = np.random.permutation(100)\n",
        "  violence_random = np.random.permutation(100)\n",
        "\n",
        "  # train\n",
        "  normal_data = np.array(os.listdir(f'{MOVIES_PATH}/Normal'))[normal_random[:70]]\n",
        "  violence_data = np.array(os.listdir(f'{MOVIES_PATH}/Violence'))[violence_random[:70]]\n",
        "\n",
        "  x, y = process_paths(normal_data, violence_data)\n",
        "\n",
        "  del normal_data\n",
        "  del violence_data\n",
        "\n",
        "  x = x.reshape(-1)\n",
        "  y = y.reshape(-1)\n",
        "\n",
        "  data_dict = {\n",
        "    \"video_name\":x,\n",
        "    \"label\":y,\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame.from_dict(data_dict)\n",
        "\n",
        "  x, y = prepare_all_videos(df)\n",
        "\n",
        "  del df\n",
        "  del data_dict\n",
        "\n",
        "  np.save(f'{save_path}/x_train',x)\n",
        "  np.save(f'{save_path}/y_train',y)\n",
        "\n",
        "  del x\n",
        "  del y\n",
        "\n",
        "  # validation\n",
        "  normal_data = np.array(os.listdir(f'{MOVIES_PATH}/Normal'))[normal_random[70:85]]\n",
        "  violence_data = np.array(os.listdir(f'{MOVIES_PATH}/Violence'))[violence_random[70:85]]\n",
        "\n",
        "  x, y = process_paths(normal_data, violence_data)\n",
        "\n",
        "  del normal_data\n",
        "  del violence_data\n",
        "\n",
        "  x = x.reshape(-1)\n",
        "  y = y.reshape(-1)\n",
        "\n",
        "  data_dict = {\n",
        "    \"video_name\":x,\n",
        "    \"label\":y,\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame.from_dict(data_dict)\n",
        "\n",
        "  x, y = prepare_all_videos(df)\n",
        "\n",
        "  del df\n",
        "  del data_dict\n",
        "\n",
        "  np.save(f'{save_path}/x_val',x)\n",
        "  np.save(f'{save_path}/y_val',y)\n",
        "\n",
        "  del x\n",
        "  del y\n",
        "\n",
        "  # test\n",
        "  normal_data = np.array(os.listdir(f'{MOVIES_PATH}/Normal'))[normal_random[85:]]\n",
        "  violence_data = np.array(os.listdir(f'{MOVIES_PATH}/Violence'))[violence_random[85:]]\n",
        "\n",
        "  x, y = process_paths(normal_data, violence_data)\n",
        "\n",
        "  del normal_data\n",
        "  del violence_data\n",
        "\n",
        "  x = x.reshape(-1)\n",
        "  y = y.reshape(-1)\n",
        "\n",
        "  data_dict = {\n",
        "    \"video_name\":x,\n",
        "    \"label\":y,\n",
        "  }\n",
        "\n",
        "  df = pd.DataFrame.from_dict(data_dict)\n",
        "\n",
        "  x, y = prepare_all_videos(df)\n",
        "\n",
        "  del df\n",
        "  del data_dict\n",
        "\n",
        "  np.save(f'{save_path}/x_test',x)\n",
        "  np.save(f'{save_path}/y_test',y)\n",
        "\n",
        "  del x\n",
        "  del y"
      ],
      "metadata": {
        "id": "0aDXp-zBy_W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_datasets(OUT_PATH)"
      ],
      "metadata": {
        "id": "nwWvZXHI1ZEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load presaved and preprocessed data"
      ],
      "metadata": {
        "id": "8GlQgzks2teN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "\n",
        "def load_x_y(\n",
        "    load_which: str = 'train', \n",
        "  ):\n",
        "  violence_data = None\n",
        "  normal_data = None\n",
        "\n",
        "  if load_which == 'train':\n",
        "    x = np.load(f'{OUT_PATH}/x_train.npy')\n",
        "    y  = np.load(f'{OUT_PATH}/y_train.npy')\n",
        "\n",
        "  elif load_which == 'validation':\n",
        "    x = np.load(f'{OUT_PATH}/x_val.npy')\n",
        "    y  = np.load(f'{OUT_PATH}/y_val.npy')\n",
        "\n",
        "  elif load_which == 'test':\n",
        "    x = np.load(f'{OUT_PATH}/x_test.npy')\n",
        "    y  = np.load(f'{OUT_PATH}/y_test.npy')\n",
        "\n",
        "  print(f'y shape = {y.shape}')\n",
        "  print(f'x shape = {x.shape}')\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "Pct8093s2nBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = load_x_y('train')\n",
        "\n",
        "x_val, y_val = load_x_y('validation')\n",
        "\n",
        "x_test, y_test = load_x_y('test')\n"
      ],
      "metadata": {
        "id": "hXsoLrYK2sMz",
        "outputId": "e9734072-4bd8-4206-c906-464ad37e2a8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape = (140,)\n",
            "x shape = (140, 20, 1024)\n",
            "y shape = (30,)\n",
            "x shape = (30, 20, 1024)\n",
            "y shape = (30,)\n",
            "x shape = (30, 20, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n"
      ],
      "metadata": {
        "id": "TzYXNhQPD-pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tfl.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = tfl.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
        "        return mask"
      ],
      "metadata": {
        "id": "paRY02FADwi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(tfl.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = tfl.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [tfl.Dense(dense_dim, activation=tf.nn.gelu), tfl.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = tfl.LayerNormalization()\n",
        "        self.layernorm_2 = tfl.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "pWEeV7IxEVW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_compiled_model():\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = 2\n",
        "\n",
        "    inputs = keras.Input(shape=(None, None))\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = tfl.GlobalMaxPooling1D()(x)\n",
        "    x = tfl.Dropout(0.5)(x)\n",
        "    outputs = tfl.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = OUT_PATH\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    model = get_compiled_model()\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        validation_data= (x_val, y_val),\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    model.load_weights(filepath)\n",
        "    _, accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "pgIGw80xE6Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40"
      ],
      "metadata": {
        "id": "gJJtdfY4B-e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, history  = run_experiment()"
      ],
      "metadata": {
        "id": "yn5F3bgOQz45",
        "outputId": "e177d4b1-9211-44e5-c95b-77c6d6bf9e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0168 - accuracy: 0.7500\n",
            "Epoch 1: val_loss improved from inf to 0.07763, saving model to /content/drive/MyDrive/automatic-detection-of-violence-in-video-scenes/transformers/preprocessed_data\n",
            "5/5 [==============================] - 4s 535ms/step - loss: 1.0168 - accuracy: 0.7500 - val_loss: 0.0776 - val_accuracy: 0.9667\n",
            "Epoch 2/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9500\n",
            "Epoch 2: val_loss improved from 0.07763 to 0.03568, saving model to /content/drive/MyDrive/automatic-detection-of-violence-in-video-scenes/transformers/preprocessed_data\n",
            "5/5 [==============================] - 2s 446ms/step - loss: 0.2175 - accuracy: 0.9500 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 3/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9714\n",
            "Epoch 3: val_loss did not improve from 0.03568\n",
            "5/5 [==============================] - 2s 393ms/step - loss: 0.1522 - accuracy: 0.9714 - val_loss: 0.4041 - val_accuracy: 0.8667\n",
            "Epoch 4/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9786\n",
            "Epoch 4: val_loss did not improve from 0.03568\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 0.0582 - accuracy: 0.9786 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
            "Epoch 5/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9929\n",
            "Epoch 5: val_loss improved from 0.03568 to 0.01985, saving model to /content/drive/MyDrive/automatic-detection-of-violence-in-video-scenes/transformers/preprocessed_data\n",
            "5/5 [==============================] - 2s 458ms/step - loss: 0.0356 - accuracy: 0.9929 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "Epoch 6/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9929\n",
            "Epoch 6: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 403ms/step - loss: 0.0124 - accuracy: 0.9929 - val_loss: 0.0585 - val_accuracy: 0.9667\n",
            "Epoch 7/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9929\n",
            "Epoch 7: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 396ms/step - loss: 0.0346 - accuracy: 0.9929 - val_loss: 0.2020 - val_accuracy: 0.9333\n",
            "Epoch 8/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9786\n",
            "Epoch 8: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 405ms/step - loss: 0.0282 - accuracy: 0.9786 - val_loss: 0.1491 - val_accuracy: 0.9333\n",
            "Epoch 9/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 445ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
            "Epoch 10/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9929\n",
            "Epoch 10: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 408ms/step - loss: 0.0201 - accuracy: 0.9929 - val_loss: 0.1228 - val_accuracy: 0.9667\n",
            "Epoch 11/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 11: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 398ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.9000\n",
            "Epoch 12/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 12: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 395ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5766 - val_accuracy: 0.9000\n",
            "Epoch 13/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.8002e-05 - accuracy: 1.0000\n",
            "Epoch 13: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 400ms/step - loss: 8.8002e-05 - accuracy: 1.0000 - val_loss: 0.5539 - val_accuracy: 0.9000\n",
            "Epoch 14/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.2348e-04 - accuracy: 1.0000\n",
            "Epoch 14: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 4.2348e-04 - accuracy: 1.0000 - val_loss: 0.5629 - val_accuracy: 0.9000\n",
            "Epoch 15/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.2302e-05 - accuracy: 1.0000\n",
            "Epoch 15: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 3.2302e-05 - accuracy: 1.0000 - val_loss: 0.5668 - val_accuracy: 0.9000\n",
            "Epoch 16/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1860e-05 - accuracy: 1.0000\n",
            "Epoch 16: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 394ms/step - loss: 2.1860e-05 - accuracy: 1.0000 - val_loss: 0.5671 - val_accuracy: 0.9000\n",
            "Epoch 17/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.5461e-04 - accuracy: 1.0000\n",
            "Epoch 17: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 402ms/step - loss: 3.5461e-04 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.9000\n",
            "Epoch 18/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.3475e-05 - accuracy: 1.0000\n",
            "Epoch 18: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 397ms/step - loss: 1.3475e-05 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 0.9000\n",
            "Epoch 19/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.2500e-04 - accuracy: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 7.2500e-04 - accuracy: 1.0000 - val_loss: 0.5442 - val_accuracy: 0.9000\n",
            "Epoch 20/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2005e-05 - accuracy: 1.0000\n",
            "Epoch 20: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 405ms/step - loss: 1.2005e-05 - accuracy: 1.0000 - val_loss: 0.4950 - val_accuracy: 0.9000\n",
            "Epoch 21/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.0982e-05 - accuracy: 1.0000\n",
            "Epoch 21: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 395ms/step - loss: 2.0982e-05 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.9000\n",
            "Epoch 22/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.6777e-05 - accuracy: 1.0000\n",
            "Epoch 22: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 400ms/step - loss: 5.6777e-05 - accuracy: 1.0000 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 23/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.6431e-06 - accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 397ms/step - loss: 7.6431e-06 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9000\n",
            "Epoch 24/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.8386e-06 - accuracy: 1.0000\n",
            "Epoch 24: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 397ms/step - loss: 7.8386e-06 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9000\n",
            "Epoch 25/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.7960e-05 - accuracy: 1.0000\n",
            "Epoch 25: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 398ms/step - loss: 4.7960e-05 - accuracy: 1.0000 - val_loss: 0.4083 - val_accuracy: 0.9000\n",
            "Epoch 26/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.6123e-06 - accuracy: 1.0000\n",
            "Epoch 26: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 3s 478ms/step - loss: 9.6123e-06 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9000\n",
            "Epoch 27/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.7418e-06 - accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 401ms/step - loss: 8.7418e-06 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.9000\n",
            "Epoch 28/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.8219e-06 - accuracy: 1.0000\n",
            "Epoch 28: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 403ms/step - loss: 5.8219e-06 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.9000\n",
            "Epoch 29/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1457e-06 - accuracy: 1.0000\n",
            "Epoch 29: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 402ms/step - loss: 2.1457e-06 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9000\n",
            "Epoch 30/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.6139e-06 - accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 395ms/step - loss: 9.6139e-06 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.9000\n",
            "Epoch 31/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.2712e-05 - accuracy: 1.0000\n",
            "Epoch 31: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 392ms/step - loss: 3.2712e-05 - accuracy: 1.0000 - val_loss: 0.4054 - val_accuracy: 0.9000\n",
            "Epoch 32/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.9054e-05 - accuracy: 1.0000\n",
            "Epoch 32: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 400ms/step - loss: 1.9054e-05 - accuracy: 1.0000 - val_loss: 0.4020 - val_accuracy: 0.9000\n",
            "Epoch 33/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.9966e-06 - accuracy: 1.0000\n",
            "Epoch 33: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 3.9966e-06 - accuracy: 1.0000 - val_loss: 0.3996 - val_accuracy: 0.9000\n",
            "Epoch 34/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.4909e-06 - accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 394ms/step - loss: 3.4909e-06 - accuracy: 1.0000 - val_loss: 0.3975 - val_accuracy: 0.9000\n",
            "Epoch 35/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.2941e-05 - accuracy: 1.0000\n",
            "Epoch 35: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 406ms/step - loss: 4.2941e-05 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.9000\n",
            "Epoch 36/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0432e-05 - accuracy: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 396ms/step - loss: 1.0432e-05 - accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9000\n",
            "Epoch 37/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.4737e-04 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 401ms/step - loss: 3.4737e-04 - accuracy: 1.0000 - val_loss: 0.4180 - val_accuracy: 0.9000\n",
            "Epoch 38/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1057e-05 - accuracy: 1.0000\n",
            "Epoch 38: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 399ms/step - loss: 2.1057e-05 - accuracy: 1.0000 - val_loss: 0.5563 - val_accuracy: 0.9000\n",
            "Epoch 39/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1295e-06 - accuracy: 1.0000\n",
            "Epoch 39: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 401ms/step - loss: 2.1295e-06 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.9000\n",
            "Epoch 40/40\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.6967e-04 - accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.01985\n",
            "5/5 [==============================] - 2s 404ms/step - loss: 1.6967e-04 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.9000\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3191 - accuracy: 0.9333\n",
            "Test accuracy: 93.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.save(OUT_PATH+'/model')"
      ],
      "metadata": {
        "id": "xKyshKegRG8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5cbe7a-da56-4eda-c04b-7f85fa2476ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, multi_head_attention_2_layer_call_fn, multi_head_attention_2_layer_call_and_return_conditional_losses, layer_normalization_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = trained_model.predict(x_test)"
      ],
      "metadata": {
        "id": "dMWsv0Z3GJV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jan6M-cxGsJv",
        "outputId": "590f80c4-ee8d-42b3-b7bc-4f4e1ad1d173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 2)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = tf.argmax(y_pred, axis=-1)"
      ],
      "metadata": {
        "id": "5ZsIn9f-G6Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r0njfVNHFBO",
        "outputId": "fc3fdb17-1941-4a99-ed0c-59647e71f395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
              "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import sqrt\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "# keep probabilities for the positive outcome only\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, y_pred)\n",
        "lr_auc = roc_auc_score(y_test, y_pred)\n",
        "# summarize scores\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "# calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "# calculate the g-mean for each threshold\n",
        "gmeans = sqrt(tpr * (1-fpr))\n",
        "# locate the index of the largest g-mean\n",
        "ix = argmax(gmeans)\n",
        "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ClWLXxPwFj0v",
        "outputId": "8dc2aee4-edeb-437e-94af-2efe316fcdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic: ROC AUC=0.933\n",
            "Best Threshold=1.000000, G-Mean=0.933\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(y_test, y_pred, pos_label=1)\n",
        "fnr = 1 - tpr\n",
        "EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
        "print('EER: %f' % EER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmjpoLRSGf_K",
        "outputId": "f5d919f2-76b0-4775-aca1-797a5ff04d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EER: 0.066667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred = [1 * (x[0]>=thresholds[ix]) for x in y_pred]\n",
        "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()"
      ],
      "metadata": {
        "id": "4die2ujXHUrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_mat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBGh9rL2Hbfm",
        "outputId": "c21d461d-68ad-4e81-80ee-b04c8c1d3286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14,  1],\n",
              "       [ 1, 14]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        " \n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = [0,1], \n",
        "                     columns = [0,1])"
      ],
      "metadata": {
        "id": "jvY6DnHpHnlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')  \n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "3C7mRPn6Hsgt",
        "outputId": "da866009-5e8b-4d0c-fa7a-c5a3908eb34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = (con_mat[0][0] + con_mat[1][1]) / (con_mat[0][0] + con_mat[0][1] + con_mat[1][0] + con_mat[1][1]) \n",
        "print('Accuracy when predicting based on best threshold: %f' % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12K27VNGHukq",
        "outputId": "99cdeac9-0793-4517-993b-4eac592ba474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy when predicting based on best threshold: 0.933333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpL0omfEHzdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}