{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: opencv-python\n",
      "Version: 4.6.0.66\n",
      "Summary: Wrapper package for OpenCV python bindings.\n",
      "Home-page: https://github.com/skvark/opencv-python\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: MIT\n",
      "Location: /anaconda/envs/azureml_py38/lib/python3.8/site-packages\n",
      "Requires: numpy, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thu4DhH6MIq1"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Videos are sequences of images. Let's assume you have an image\n",
    "representation model (CNN, ViT, etc.) and a sequence model\n",
    "(RNN, LSTM, etc.) at hand. We ask you to tweak the model for video\n",
    "classification. The simplest approach would be to apply the image\n",
    "model to individual frames, use the sequence model to learn\n",
    "sequences of image features, then apply a classification head on\n",
    "the learned sequence representation.\n",
    "The Keras example\n",
    "[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\n",
    "explains this approach in detail. Alernatively, you can also\n",
    "build a hybrid Transformer-based model for video classification as shown in the Keras example\n",
    "[Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n",
    "\n",
    "In this example, we minimally implement\n",
    "[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)\n",
    "by Arnab et al., a **pure Transformer-based** model\n",
    "for video classification. The authors propose a novel embedding scheme\n",
    "and a number of Transformer variants to model video clips. We implement\n",
    "the embedding scheme and one of the variants of the Transformer\n",
    "architecture, for simplicity.\n",
    "\n",
    "This example requires TensorFlow 2.6 or higher, and the `medmnist`\n",
    "package, which can be installed by running the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IQLjkXIMIq4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LhaEi9ToMIq5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import imageio\n",
    "import medmnist\n",
    "#import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhnaMHXmMIq6"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "The hyperparameters are chosen via hyperparameter\n",
    "search. You can learn more about the process in the \"conclusion\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ebygdrVVMIq7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import imageio\n",
    "import medmnist\n",
    "#import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# DATA\n",
    "DATASET_NAME = \"RWF-2000\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 2\n",
    "train_ratio = 0.7\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 25\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../datasets/RWF-2000/RWF-2000/train'\n",
    "\n",
    "Fight_PATH = ROOT_PATH + '/Fight'\n",
    "Non_Fight_PATH = ROOT_PATH + '/NonFight'\n",
    "\n",
    "OUT_PATH = '../datasets/RWF-2000/RWF-2000/Extracted frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(directory, dimensions=(INPUT_SHAPE[1], INPUT_SHAPE[2]), packet_length = INPUT_SHAPE[0], save_dir_path = None):\n",
    "    \"\"\" \n",
    "    Extract packets from video directory\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    directory : str\n",
    "    A directory on the disk that contains the videos \n",
    "\n",
    "    dimensions: Tuple of shape 2 , optional\n",
    "    The desired frame dimensions to be stored\n",
    "    default = (50,50)\n",
    "\n",
    "    packet_length: int , optional\n",
    "    Packet length , Default = 15\n",
    "\n",
    "    save_dir : str , optional\n",
    "    The path to which the axtracted data should be saved\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray \n",
    "    The extracted packets from each video stacked together \n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for video_name in os.listdir(directory):\n",
    "        video = cv2.VideoCapture(directory + '/' + video_name)\n",
    "        packet = []\n",
    "        i = 0\n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "\n",
    "            if not ret: # no more frames\n",
    "                break\n",
    "\n",
    "            del ret\n",
    "\n",
    "    #         if i % 2 == 0: # capture one in every 2 frames\n",
    "    #             i += 1\n",
    "    #             continue\n",
    "\n",
    "            # capturing the frame\n",
    "\n",
    "            frame = cv2.resize(frame, dimensions, interpolation = cv2.INTER_AREA)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            packet.append(frame)\n",
    "            del frame\n",
    "\n",
    "            if len(packet) == packet_length: \n",
    "                '''\n",
    "                consecutive packets share 14 of the 15 frames to generate more data\n",
    "                '''\n",
    "                # packet itself is not normalized\n",
    "                stacked = np.array(packet) # convert to numpy and normalize packet\n",
    "                data.append(stacked.copy()) # this .copy() is not in the original code \n",
    "                packet.pop(0) \n",
    "\n",
    "            i += 1\n",
    "\n",
    "        video.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    del packet\n",
    "    # TODO: read the docs for the next 2 lines\n",
    "    data = np.stack(data, axis= 0)\n",
    "    data = np.moveaxis(data, 1 ,-1)\n",
    "\n",
    "    # save to disk\n",
    "    if(save_dir_path is not None):\n",
    "        np.save(f'{save_dir_path}/data', data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_frames = np.load('../datasets/RWF-2000/RWF-2000/Extracted frames/violence.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fight_frames = np.load('../datasets/RWF-2000/RWF-2000/Extracted frames/non-violence.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fight_frames = extract_frames(Fight_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98400, 28, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fight_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_fight_frames = extract_frames(Non_Fight_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98400, 28, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_fight_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../datasets/RWF-2000/RWF-2000/Extracted frames/non-violence', non_fight_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../datasets/RWF-2000/RWF-2000/Extracted frames/violence', fight_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = int(train_ratio*fight_frames.shape[0])\n",
    "train_nd_fight_videos, train_nd_fight_labels = fight_frames[:n_train_samples], np.full((n_train_samples,), 0)\n",
    "\n",
    "n_valid_samples = fight_frames.shape[0] - n_train_samples\n",
    "valid_nd_fight_videos, valid_nd_fight_labels = fight_frames[n_train_samples:], np.full((n_valid_samples,), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68880, 28, 28, 28), (68880,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nd_fight_videos.shape, train_nd_fight_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29520, 28, 28, 28), (29520,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_nd_fight_videos.shape, valid_nd_fight_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = int(train_ratio*non_fight_frames.shape[0])\n",
    "train_nd_non_fight_videos, train_nd_non_fight_labels = non_fight_frames[:n_train_samples], np.full((n_train_samples,), 1)\n",
    "\n",
    "n_valid_samples = non_fight_frames.shape[0] - n_train_samples\n",
    "valid_nd_non_fight_videos, valid_nd_non_fight_labels = non_fight_frames[n_train_samples:], np.full((n_valid_samples,), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68880, 28, 28, 28), (68880,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nd_non_fight_videos.shape, train_nd_non_fight_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29520, 28, 28, 28), (29520,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_nd_non_fight_videos.shape, valid_nd_non_fight_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nd_videos = np.concatenate((train_nd_fight_videos, train_nd_non_fight_videos), axis = 0)\n",
    "valid_nd_videos = np.concatenate((valid_nd_fight_videos, valid_nd_non_fight_videos), axis = 0)\n",
    "\n",
    "train_nd_labels = np.concatenate((train_nd_fight_labels, train_nd_non_fight_labels), axis = 0)\n",
    "valid_nd_labels = np.concatenate((valid_nd_fight_labels, valid_nd_non_fight_labels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((137760, 28, 28, 28), (137760,), (59040, 28, 28, 28), (59040,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nd_videos.shape, train_nd_labels.shape, valid_nd_videos.shape, valid_nd_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_XGA2IZMIq-"
   },
   "source": [
    "### `tf.data` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "niqEYZjpMIq-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:01:38.217950: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-18 17:01:38.218032: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gradinstance1): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 17:01:38.238894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 17:01:43.444163: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3024107520 exceeds 10% of free system memory.\n",
      "2022-11-18 17:01:47.471813: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1296046080 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_nd_videos, train_nd_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_nd_videos, valid_nd_labels, \"valid\")\n",
    "# testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XrogaBGMIq_"
   },
   "source": [
    "## Tubelet Embedding\n",
    "\n",
    "In ViTs, an image is divided into patches, which are then spatially\n",
    "flattened, a process known as tokenization. For a video, one can\n",
    "repeat this process for individual frames. **Uniform frame sampling**\n",
    "as suggested by the authors is a tokenization scheme in which we\n",
    "sample frames from the video clip and perform simple ViT tokenization.\n",
    "\n",
    "| ![uniform frame sampling](https://i.imgur.com/aaPyLPX.png) |\n",
    "| :--: |\n",
    "| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n",
    "\n",
    "**Tubelet Embedding** is different in terms of capturing temporal\n",
    "information from the video.\n",
    "First, we extract volumes from the video -- these volumes contain\n",
    "patches of the frame and the temporal information as well. The volumes\n",
    "are then flattened to build video tokens.\n",
    "\n",
    "| ![tubelet embedding](https://i.imgur.com/9G7QTfV.png) |\n",
    "| :--: |\n",
    "| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "igqdpz27MIq_"
   },
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Em2YiFFMIrA"
   },
   "source": [
    "## Positional Embedding\n",
    "\n",
    "This layer adds positional information to the encoded video tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9P4kswaIMIrA"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFcUPSLhMIrB"
   },
   "source": [
    "## Video Vision Transformer\n",
    "\n",
    "The authors suggest 4 variants of Vision Transformer:\n",
    "\n",
    "- Spatio-temporal attention\n",
    "- Factorized encoder\n",
    "- Factorized self-attention\n",
    "- Factorized dot-product attention\n",
    "\n",
    "In this example, we will implement the **Spatio-temporal attention**\n",
    "model for simplicity. The following code snippet is heavily inspired from\n",
    "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
    "One can also refer to the\n",
    "[official repository of ViViT](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)\n",
    "which contains all the variants, implemented in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tIWQUMvdMIrB"
   },
   "outputs": [],
   "source": [
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint('../checkpoints/RWF-2000',\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "              min_delta=0,\n",
    "              patience=2,\n",
    "              restore_best_weights=True)\n",
    "callbacks = [model_checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmGYIJZaMIrC"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "L2mQonRkMIrC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:04:54.988149: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_UINT8\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 137760\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-11-18 17:04:56.107376: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:14:59.740907: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_UINT8\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 59040\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:5\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_UINT8\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-11-18 17:15:00.247522: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 696s 159ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 5.7823 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "   1/4305 [..............................] - ETA: 11:26 - loss: 11.5677 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:16:34.599218: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:26:27.615690: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 683s 159ms/step - loss: 0.0217 - accuracy: 0.9969 - val_loss: 6.3946 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "   1/4305 [..............................] - ETA: 12:50 - loss: 12.6368 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:27:58.572073: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:37:40.307121: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 678s 157ms/step - loss: 0.0315 - accuracy: 0.9948 - val_loss: 4.8316 - val_accuracy: 0.5000\n",
      "Epoch 4/25\n",
      "   1/4305 [..............................] - ETA: 11:54 - loss: 9.5706 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:39:17.339260: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9929"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:48:58.267669: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 670s 156ms/step - loss: 0.0357 - accuracy: 0.9929 - val_loss: 5.1857 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "   1/4305 [..............................] - ETA: 12:02 - loss: 10.1596 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:50:28.344977: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9911"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:00:01.548582: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 661s 154ms/step - loss: 0.0455 - accuracy: 0.9911 - val_loss: 5.0615 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "   1/4305 [..............................] - ETA: 11:08 - loss: 9.8571 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:01:31.150858: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9897"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:11:02.796236: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4305/4305 [==============================] - 661s 154ms/step - loss: 0.0511 - accuracy: 0.9897 - val_loss: 4.6632 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "   1/4305 [..............................] - ETA: 11:37 - loss: 9.1498 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:12:34.107283: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3612/4305 [========================>.....] - ETA: 1:31 - loss: 0.0687 - accuracy: 0.9855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/workspace/code/vivit_new.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m#     _, accuracy, top_5_accuracy = model.evaluate(testloader)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m#     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m run_experiment()\n",
      "\u001b[1;32m/home/azureuser/workspace/code/vivit_new.ipynb Cell 36\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         metrics\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mSparseCategoricalAccuracy(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Train the model.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(trainloader, epochs\u001b[39m=\u001b[39;49mEPOCHS, validation_data\u001b[39m=\u001b[39;49mvalidloader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     _, accuracy, top_5_accuracy = model.evaluate(testloader)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2247726164417a757265227d/home/azureuser/workspace/code/vivit_new.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment():\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "    with mirrored_strategy.scope():\n",
    "    \n",
    "        # Initialize model\n",
    "        model = create_vivit_classifier(\n",
    "            tubelet_embedder=TubeletEmbedding(\n",
    "                embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "            ),\n",
    "            positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "        )\n",
    "\n",
    "        # Compile the model with the optimizer, loss function\n",
    "        # and the metrics.\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "        )\n",
    "\n",
    "        # Train the model.\n",
    "\n",
    "        _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    #     _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    #     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    #     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFGwukzfMIrD"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjMUP-SHMIrD"
   },
   "outputs": [],
   "source": [
    "# NUM_SAMPLES_VIZ = 25\n",
    "# testsamples, labels = next(iter(testloader))\n",
    "# testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "# ground_truths = []\n",
    "# preds = []\n",
    "# videos = []\n",
    "\n",
    "# for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "#     # Generate gif\n",
    "#     with io.BytesIO() as gif:\n",
    "#         imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
    "#         videos.append(gif.getvalue())\n",
    "\n",
    "#     # Get model prediction\n",
    "#     output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "#     pred = np.argmax(output, axis=0)\n",
    "\n",
    "#     ground_truths.append(label.numpy().astype(\"int\"))\n",
    "#     preds.append(pred)\n",
    "\n",
    "\n",
    "# def make_box_for_grid(image_widget, fit):\n",
    "#     \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "\n",
    "#     Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
    "#     \"\"\"\n",
    "#     # Make the caption\n",
    "#     if fit is not None:\n",
    "#         fit_str = \"'{}'\".format(fit)\n",
    "#     else:\n",
    "#         fit_str = str(fit)\n",
    "\n",
    "#     h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "#     # Make the green box with the image widget inside it\n",
    "#     boxb = ipywidgets.widgets.Box()\n",
    "#     boxb.children = [image_widget]\n",
    "\n",
    "#     # Compose into a vertical box\n",
    "#     vb = ipywidgets.widgets.VBox()\n",
    "#     vb.layout.align_items = \"center\"\n",
    "#     vb.children = [h, boxb]\n",
    "#     return vb\n",
    "\n",
    "\n",
    "# boxes = []\n",
    "# for i in range(NUM_SAMPLES_VIZ):\n",
    "#     ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "#     true_class = info[\"label\"][str(ground_truths[i])]\n",
    "#     pred_class = info[\"label\"][str(preds[i])]\n",
    "#     caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "#     boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "# ipywidgets.widgets.GridBox(\n",
    "#     boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc9pqfc8MIrE"
   },
   "source": [
    "## Final thoughts\n",
    "\n",
    "With a vanilla implementation, we achieve ~79-80% Top-1 accuracy on the\n",
    "test dataset.\n",
    "\n",
    "The hyperparameters used in this tutorial were finalized by running a\n",
    "hyperparameter search using\n",
    "[W&B Sweeps](https://docs.wandb.ai/guides/sweeps).\n",
    "You can find out our sweeps result\n",
    "[here](https://wandb.ai/minimal-implementations/vivit/sweeps/66fp0lhz)\n",
    "and our quick analysis of the results\n",
    "[here](https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx).\n",
    "\n",
    "For further improvement, you could look into the following:\n",
    "\n",
    "- Using data augmentation for videos.\n",
    "- Using a better regularization scheme for training.\n",
    "- Apply different variants of the transformer model as in the paper.\n",
    "\n",
    "We would like to thank [Anurag Arnab](https://anuragarnab.github.io/)\n",
    "(first author of ViViT) for helpful discussion. We are grateful to\n",
    "[Weights and Biases](https://wandb.ai/site) program for helping with\n",
    "GPU credits.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/video-vision-transformer) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/video-vision-transformer-CT)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vivit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
