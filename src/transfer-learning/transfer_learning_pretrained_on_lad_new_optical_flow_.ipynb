{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Note: Check paths before running\n",
        "\n"
      ],
      "metadata": {
        "id": "-kIPzYrYxEis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "FmPADVV4uv2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import ipywidgets\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils import to_categorical, OrderedEnqueuer\n",
        "\n",
        "import ipywidgets\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback, WandbMetricsLogger\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "sbvx0xxEuvXi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "tf.test.is_built_with_cuda()\n",
        "print(tf.version.VERSION)\n",
        "import sys\n",
        "sys.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "_5nNyGk-KuPb",
        "outputId": "a54abca3-87b7-45c6-f110-b6b0a9976732"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "2.12.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drive mounting"
      ],
      "metadata": {
        "id": "tgDV5Dsau2jI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kgJrQtAueAi",
        "outputId": "fa0a913c-1a5f-433f-efed-098502bfc485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters"
      ],
      "metadata": {
        "id": "VE8Z4kWN29w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (75, 75, 14, 2)\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = 10\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 510\n",
        "NUM_HEADS = 17\n",
        "NUM_LAYERS = 9"
      ],
      "metadata": {
        "id": "VPCXXWYPqr2M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Generator"
      ],
      "metadata": {
        "id": "c1F8slLE1zCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_in, y_in, batch_size=BATCH_SIZE, shuffle=True):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.x = x_in\n",
        "        self.y = y_in\n",
        "        self.datalen = len(y_in)\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get batch indexes from shuffled indexes\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x_batch = self.x[batch_indexes]\n",
        "        y_batch = self.y[batch_indexes]\n",
        "        return x_batch, y_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch\n",
        "        return self.datalen // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updates indexes after each epoch\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "uImgcpBQ1xLA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross testing the trained model"
      ],
      "metadata": {
        "id": "Pfk8ppEbvICg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Data"
      ],
      "metadata": {
        "id": "HHR0pjbEvDBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH_UCF = '/content/gdrive/MyDrive/Vio_Nor_Binary_data/'"
      ],
      "metadata": {
        "id": "0_pnWJ4ov-Wo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LAD2000"
      ],
      "metadata": {
        "id": "IAxr0hH_xfuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ucf, y_train_ucf = np.load(DATA_PATH_UCF+ 'Train/X_train_dense_flow.npy',mmap_mode='r'), np.load(DATA_PATH_UCF + 'Train/y_train_dense_flow.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "HyErOfAswqKC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf, y_test_ucf = np.load(DATA_PATH_UCF+ 'Test/X_test_dense_flow.npy',mmap_mode='r'), np.load(DATA_PATH_UCF+ 'Test/y_test_dense_flow.npy',mmap_mode='r')"
      ],
      "metadata": {
        "id": "uZkIwLOuxa97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ucf, y_val_ucf = np.load(DATA_PATH_UCF + 'Val/X_val_dense_flow.npy',mmap_mode='r'), np.load(DATA_PATH_UCF + 'Val/y_val_dense_flow.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "0QJEaT3gxa7Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ucf.shape, y_train_ucf.shape"
      ],
      "metadata": {
        "id": "LZAYi87Txayk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81cbeb7-2618-4ba5-b738-b5b89cc1ac2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4573, 75, 75, 14, 2), (4573,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf.shape, y_test_ucf.shape"
      ],
      "metadata": {
        "id": "wQrlTfstxarv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615bda74-5daa-4869-8d72-150bfcbe97b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1345, 75, 75, 14, 2), (1345,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ucf.shape, y_val_ucf.shape"
      ],
      "metadata": {
        "id": "TO8oH0u1xwao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7646397b-0728-4490-e5f7-79dfc1b8f31d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((655, 75, 75, 14, 2), (655,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Labels"
      ],
      "metadata": {
        "id": "ZuNCTOc61vpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_ucf = to_categorical(y_train_ucf, 2)\n",
        "y_val_ucf = to_categorical(y_val_ucf, 2)\n",
        "y_test_ucf = to_categorical(y_test_ucf, 2)"
      ],
      "metadata": {
        "id": "vtcJPoRZ1xSS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = DataGenerator(X_train_ucf, y_train_ucf)\n",
        "validation_generator = DataGenerator(X_val_ucf, y_val_ucf)"
      ],
      "metadata": {
        "id": "v8uDkglHOAsw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ViVit Model\n"
      ],
      "metadata": {
        "id": "PPKHIq1G2Y8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ],
      "metadata": {
        "id": "gpL6HxWM2YZ2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "6nzu1V2O2hD8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    transformer_layers,\n",
        "    num_heads,\n",
        "    embed_dim,\n",
        "    dropout,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=2,\n",
        "):\n",
        "    input_shape = INPUT_SHAPE\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "R_mQLRT_2jlp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the Model"
      ],
      "metadata": {
        "id": "jQxSEmqox0K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = '/content/gdrive/MyDrive/Checkpoints/Checkpoints/hptuning-OF-lad2000-vivit/checkpoint.tf'"
      ],
      "metadata": {
        "id": "QKHAQQw1vHbg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "                embed_dim=121, patch_size=(\n",
        "                    3,3,3)\n",
        "            ),\n",
        "            positional_encoder=PositionalEncoder(embed_dim=121),\n",
        "        transformer_layers=7,\n",
        "        num_heads=2,\n",
        "        embed_dim=121,\n",
        "        dropout=0.3,\n",
        "    )\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ol2HC0jEuILW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(MODEL_PATH)"
      ],
      "metadata": {
        "id": "NebVaS-w1T08"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGS5KV9U0-Xr",
        "outputId": "26e92f6d-477e-427d-c17f-7bb19dc030d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 75, 75, 14,  0           []                               \n",
            "                                 2)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding (TubeletEmbe  (None, 81, 30)      30750       ['input_1[0][0]']                \n",
            " dding)                                                                                           \n",
            "                                                                                                  \n",
            " positional_encoder (Positional  (None, 81, 30)      2430        ['tubelet_embedding[0][0]']      \n",
            " Encoder)                                                                                         \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 81, 30)      60          ['positional_encoder[0][0]']     \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 81, 30)      2121        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 81, 30)       0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'positional_encoder[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 81, 30)      60          ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 81, 30)       7350        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 81, 30)       0           ['sequential[0][0]',             \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 81, 30)      60          ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 81, 30)      2121        ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 81, 30)       0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 81, 30)      60          ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 81, 30)       0           ['sequential_1[0][0]',           \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 81, 30)      60          ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 81, 30)      2121        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 81, 30)       0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 81, 30)      60          ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 81, 30)       0           ['sequential_2[0][0]',           \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 81, 30)      60          ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 81, 30)      2121        ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 81, 30)       0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 81, 30)      60          ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 81, 30)       0           ['sequential_3[0][0]',           \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 81, 30)      60          ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 81, 30)      2121        ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 81, 30)       0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 81, 30)      60          ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 81, 30)       0           ['sequential_4[0][0]',           \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 81, 30)      60          ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 81, 30)      2121        ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 81, 30)       0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 81, 30)      60          ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 81, 30)       0           ['sequential_5[0][0]',           \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 81, 30)      60          ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 81, 30)      2121        ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 81, 30)       0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 81, 30)      60          ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_6 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 81, 30)       0           ['sequential_6[0][0]',           \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 81, 30)      60          ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 81, 30)      2121        ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 81, 30)       0           ['multi_head_attention_7[0][0]', \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 81, 30)      60          ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_7 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 81, 30)       0           ['sequential_7[0][0]',           \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 81, 30)      60          ['add_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 81, 30)      2121        ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 81, 30)       0           ['multi_head_attention_8[0][0]', \n",
            "                                                                  'add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 81, 30)      60          ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_8 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 81, 30)       0           ['sequential_8[0][0]',           \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 81, 30)      60          ['add_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 81, 30)      2121        ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 81, 30)       0           ['multi_head_attention_9[0][0]', \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 81, 30)      60          ['add_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_9 (Sequential)      (None, 81, 30)       7350        ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 81, 30)       0           ['sequential_9[0][0]',           \n",
            "                                                                  'add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 81, 30)      60          ['add_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 81, 30)      2121        ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 81, 30)       0           ['multi_head_attention_10[0][0]',\n",
            "                                                                  'add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 81, 30)      60          ['add_20[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_10 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 81, 30)       0           ['sequential_10[0][0]',          \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 81, 30)      60          ['add_21[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 81, 30)      2121        ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 81, 30)       0           ['multi_head_attention_11[0][0]',\n",
            "                                                                  'add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 81, 30)      60          ['add_22[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_11 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 81, 30)       0           ['sequential_11[0][0]',          \n",
            "                                                                  'add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 81, 30)      60          ['add_23[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 81, 30)      2121        ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 81, 30)       0           ['multi_head_attention_12[0][0]',\n",
            "                                                                  'add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 81, 30)      60          ['add_24[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_12 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 81, 30)       0           ['sequential_12[0][0]',          \n",
            "                                                                  'add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 81, 30)      60          ['add_25[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 81, 30)      2121        ['layer_normalization_26[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 81, 30)       0           ['multi_head_attention_13[0][0]',\n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 81, 30)      60          ['add_26[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_13 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 81, 30)       0           ['sequential_13[0][0]',          \n",
            "                                                                  'add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 81, 30)      60          ['add_27[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 81, 30)      2121        ['layer_normalization_28[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 81, 30)       0           ['multi_head_attention_14[0][0]',\n",
            "                                                                  'add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 81, 30)      60          ['add_28[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_14 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 81, 30)       0           ['sequential_14[0][0]',          \n",
            "                                                                  'add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 81, 30)      60          ['add_29[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 81, 30)      2121        ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 81, 30)       0           ['multi_head_attention_15[0][0]',\n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 81, 30)      60          ['add_30[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_15 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 81, 30)       0           ['sequential_15[0][0]',          \n",
            "                                                                  'add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 81, 30)      60          ['add_31[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (Multi  (None, 81, 30)      2121        ['layer_normalization_32[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 81, 30)       0           ['multi_head_attention_16[0][0]',\n",
            "                                                                  'add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 81, 30)      60          ['add_32[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_16 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 81, 30)       0           ['sequential_16[0][0]',          \n",
            "                                                                  'add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 81, 30)      60          ['add_33[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (Multi  (None, 81, 30)      2121        ['layer_normalization_34[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 81, 30)       0           ['multi_head_attention_17[0][0]',\n",
            "                                                                  'add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 81, 30)      60          ['add_34[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_17 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 81, 30)       0           ['sequential_17[0][0]',          \n",
            "                                                                  'add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_36 (LayerN  (None, 81, 30)      60          ['add_35[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (Multi  (None, 81, 30)      2121        ['layer_normalization_36[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " add_36 (Add)                   (None, 81, 30)       0           ['multi_head_attention_18[0][0]',\n",
            "                                                                  'add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_37 (LayerN  (None, 81, 30)      60          ['add_36[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_18 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " add_37 (Add)                   (None, 81, 30)       0           ['sequential_18[0][0]',          \n",
            "                                                                  'add_36[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_38 (LayerN  (None, 81, 30)      60          ['add_37[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (Multi  (None, 81, 30)      2121        ['layer_normalization_38[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_38 (Add)                   (None, 81, 30)       0           ['multi_head_attention_19[0][0]',\n",
            "                                                                  'add_37[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_39 (LayerN  (None, 81, 30)      60          ['add_38[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_19 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " add_39 (Add)                   (None, 81, 30)       0           ['sequential_19[0][0]',          \n",
            "                                                                  'add_38[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_40 (LayerN  (None, 81, 30)      60          ['add_39[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (Multi  (None, 81, 30)      2121        ['layer_normalization_40[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " add_40 (Add)                   (None, 81, 30)       0           ['multi_head_attention_20[0][0]',\n",
            "                                                                  'add_39[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_41 (LayerN  (None, 81, 30)      60          ['add_40[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_20 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_41 (Add)                   (None, 81, 30)       0           ['sequential_20[0][0]',          \n",
            "                                                                  'add_40[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_42 (LayerN  (None, 81, 30)      60          ['add_41[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (Multi  (None, 81, 30)      2121        ['layer_normalization_42[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " add_42 (Add)                   (None, 81, 30)       0           ['multi_head_attention_21[0][0]',\n",
            "                                                                  'add_41[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_43 (LayerN  (None, 81, 30)      60          ['add_42[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_21 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " add_43 (Add)                   (None, 81, 30)       0           ['sequential_21[0][0]',          \n",
            "                                                                  'add_42[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_44 (LayerN  (None, 81, 30)      60          ['add_43[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (Multi  (None, 81, 30)      2121        ['layer_normalization_44[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_44 (Add)                   (None, 81, 30)       0           ['multi_head_attention_22[0][0]',\n",
            "                                                                  'add_43[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_45 (LayerN  (None, 81, 30)      60          ['add_44[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_22 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " add_45 (Add)                   (None, 81, 30)       0           ['sequential_22[0][0]',          \n",
            "                                                                  'add_44[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_46 (LayerN  (None, 81, 30)      60          ['add_45[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (Multi  (None, 81, 30)      2121        ['layer_normalization_46[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " add_46 (Add)                   (None, 81, 30)       0           ['multi_head_attention_23[0][0]',\n",
            "                                                                  'add_45[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_47 (LayerN  (None, 81, 30)      60          ['add_46[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_23 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " add_47 (Add)                   (None, 81, 30)       0           ['sequential_23[0][0]',          \n",
            "                                                                  'add_46[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_48 (LayerN  (None, 81, 30)      60          ['add_47[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_24 (Multi  (None, 81, 30)      2121        ['layer_normalization_48[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " add_48 (Add)                   (None, 81, 30)       0           ['multi_head_attention_24[0][0]',\n",
            "                                                                  'add_47[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_49 (LayerN  (None, 81, 30)      60          ['add_48[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_24 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " add_49 (Add)                   (None, 81, 30)       0           ['sequential_24[0][0]',          \n",
            "                                                                  'add_48[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_50 (LayerN  (None, 81, 30)      60          ['add_49[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_25 (Multi  (None, 81, 30)      2121        ['layer_normalization_50[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " add_50 (Add)                   (None, 81, 30)       0           ['multi_head_attention_25[0][0]',\n",
            "                                                                  'add_49[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_51 (LayerN  (None, 81, 30)      60          ['add_50[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_25 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 81, 30)       0           ['sequential_25[0][0]',          \n",
            "                                                                  'add_50[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_52 (LayerN  (None, 81, 30)      60          ['add_51[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_26 (Multi  (None, 81, 30)      2121        ['layer_normalization_52[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 81, 30)       0           ['multi_head_attention_26[0][0]',\n",
            "                                                                  'add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_53 (LayerN  (None, 81, 30)      60          ['add_52[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_26 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 81, 30)       0           ['sequential_26[0][0]',          \n",
            "                                                                  'add_52[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_54 (LayerN  (None, 81, 30)      60          ['add_53[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 81, 30)      2121        ['layer_normalization_54[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 81, 30)       0           ['multi_head_attention_27[0][0]',\n",
            "                                                                  'add_53[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_55 (LayerN  (None, 81, 30)      60          ['add_54[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_27 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 81, 30)       0           ['sequential_27[0][0]',          \n",
            "                                                                  'add_54[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_56 (LayerN  (None, 81, 30)      60          ['add_55[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_28 (Multi  (None, 81, 30)      2121        ['layer_normalization_56[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 81, 30)       0           ['multi_head_attention_28[0][0]',\n",
            "                                                                  'add_55[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_57 (LayerN  (None, 81, 30)      60          ['add_56[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_28 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 81, 30)       0           ['sequential_28[0][0]',          \n",
            "                                                                  'add_56[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_58 (LayerN  (None, 81, 30)      60          ['add_57[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_29 (Multi  (None, 81, 30)      2121        ['layer_normalization_58[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " add_58 (Add)                   (None, 81, 30)       0           ['multi_head_attention_29[0][0]',\n",
            "                                                                  'add_57[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_59 (LayerN  (None, 81, 30)      60          ['add_58[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_29 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " add_59 (Add)                   (None, 81, 30)       0           ['sequential_29[0][0]',          \n",
            "                                                                  'add_58[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_60 (LayerN  (None, 81, 30)      60          ['add_59[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_30 (Multi  (None, 81, 30)      2121        ['layer_normalization_60[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 81, 30)       0           ['multi_head_attention_30[0][0]',\n",
            "                                                                  'add_59[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_61 (LayerN  (None, 81, 30)      60          ['add_60[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_30 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 81, 30)       0           ['sequential_30[0][0]',          \n",
            "                                                                  'add_60[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_62 (LayerN  (None, 81, 30)      60          ['add_61[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_31 (Multi  (None, 81, 30)      2121        ['layer_normalization_62[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 81, 30)       0           ['multi_head_attention_31[0][0]',\n",
            "                                                                  'add_61[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_63 (LayerN  (None, 81, 30)      60          ['add_62[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_31 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 81, 30)       0           ['sequential_31[0][0]',          \n",
            "                                                                  'add_62[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_64 (LayerN  (None, 81, 30)      60          ['add_63[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, 81, 30)      2121        ['layer_normalization_64[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 81, 30)       0           ['multi_head_attention_32[0][0]',\n",
            "                                                                  'add_63[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_65 (LayerN  (None, 81, 30)      60          ['add_64[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_32 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 81, 30)       0           ['sequential_32[0][0]',          \n",
            "                                                                  'add_64[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_66 (LayerN  (None, 81, 30)      60          ['add_65[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 81, 30)      2121        ['layer_normalization_66[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 81, 30)       0           ['multi_head_attention_33[0][0]',\n",
            "                                                                  'add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_67 (LayerN  (None, 81, 30)      60          ['add_66[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_33 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 81, 30)       0           ['sequential_33[0][0]',          \n",
            "                                                                  'add_66[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_68 (LayerN  (None, 81, 30)      60          ['add_67[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 81, 30)      2121        ['layer_normalization_68[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 81, 30)       0           ['multi_head_attention_34[0][0]',\n",
            "                                                                  'add_67[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_69 (LayerN  (None, 81, 30)      60          ['add_68[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_34 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 81, 30)       0           ['sequential_34[0][0]',          \n",
            "                                                                  'add_68[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_70 (LayerN  (None, 81, 30)      60          ['add_69[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 81, 30)      2121        ['layer_normalization_70[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 81, 30)       0           ['multi_head_attention_35[0][0]',\n",
            "                                                                  'add_69[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_71 (LayerN  (None, 81, 30)      60          ['add_70[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_35 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 81, 30)       0           ['sequential_35[0][0]',          \n",
            "                                                                  'add_70[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_72 (LayerN  (None, 81, 30)      60          ['add_71[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_36 (Multi  (None, 81, 30)      2121        ['layer_normalization_72[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 81, 30)       0           ['multi_head_attention_36[0][0]',\n",
            "                                                                  'add_71[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_73 (LayerN  (None, 81, 30)      60          ['add_72[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_36 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 81, 30)       0           ['sequential_36[0][0]',          \n",
            "                                                                  'add_72[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_74 (LayerN  (None, 81, 30)      60          ['add_73[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 81, 30)      2121        ['layer_normalization_74[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 81, 30)       0           ['multi_head_attention_37[0][0]',\n",
            "                                                                  'add_73[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_75 (LayerN  (None, 81, 30)      60          ['add_74[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_37 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " add_75 (Add)                   (None, 81, 30)       0           ['sequential_37[0][0]',          \n",
            "                                                                  'add_74[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_76 (LayerN  (None, 81, 30)      60          ['add_75[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 81, 30)      2121        ['layer_normalization_76[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " add_76 (Add)                   (None, 81, 30)       0           ['multi_head_attention_38[0][0]',\n",
            "                                                                  'add_75[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_77 (LayerN  (None, 81, 30)      60          ['add_76[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_38 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " add_77 (Add)                   (None, 81, 30)       0           ['sequential_38[0][0]',          \n",
            "                                                                  'add_76[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_78 (LayerN  (None, 81, 30)      60          ['add_77[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 81, 30)      2121        ['layer_normalization_78[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 81, 30)       0           ['multi_head_attention_39[0][0]',\n",
            "                                                                  'add_77[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_79 (LayerN  (None, 81, 30)      60          ['add_78[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_39 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 81, 30)       0           ['sequential_39[0][0]',          \n",
            "                                                                  'add_78[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_80 (LayerN  (None, 81, 30)      60          ['add_79[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 81, 30)      2121        ['layer_normalization_80[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " add_80 (Add)                   (None, 81, 30)       0           ['multi_head_attention_40[0][0]',\n",
            "                                                                  'add_79[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_81 (LayerN  (None, 81, 30)      60          ['add_80[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_40 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " add_81 (Add)                   (None, 81, 30)       0           ['sequential_40[0][0]',          \n",
            "                                                                  'add_80[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_82 (LayerN  (None, 81, 30)      60          ['add_81[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 81, 30)      2121        ['layer_normalization_82[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " add_82 (Add)                   (None, 81, 30)       0           ['multi_head_attention_41[0][0]',\n",
            "                                                                  'add_81[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_83 (LayerN  (None, 81, 30)      60          ['add_82[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_41 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " add_83 (Add)                   (None, 81, 30)       0           ['sequential_41[0][0]',          \n",
            "                                                                  'add_82[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 81, 30)      60          ['add_83[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (Multi  (None, 81, 30)      2121        ['layer_normalization_84[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " add_84 (Add)                   (None, 81, 30)       0           ['multi_head_attention_42[0][0]',\n",
            "                                                                  'add_83[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_85 (LayerN  (None, 81, 30)      60          ['add_84[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_42 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " add_85 (Add)                   (None, 81, 30)       0           ['sequential_42[0][0]',          \n",
            "                                                                  'add_84[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_86 (LayerN  (None, 81, 30)      60          ['add_85[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (Multi  (None, 81, 30)      2121        ['layer_normalization_86[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " add_86 (Add)                   (None, 81, 30)       0           ['multi_head_attention_43[0][0]',\n",
            "                                                                  'add_85[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_87 (LayerN  (None, 81, 30)      60          ['add_86[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_43 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " add_87 (Add)                   (None, 81, 30)       0           ['sequential_43[0][0]',          \n",
            "                                                                  'add_86[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_88 (LayerN  (None, 81, 30)      60          ['add_87[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_44 (Multi  (None, 81, 30)      2121        ['layer_normalization_88[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " add_88 (Add)                   (None, 81, 30)       0           ['multi_head_attention_44[0][0]',\n",
            "                                                                  'add_87[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_89 (LayerN  (None, 81, 30)      60          ['add_88[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_44 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " add_89 (Add)                   (None, 81, 30)       0           ['sequential_44[0][0]',          \n",
            "                                                                  'add_88[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 81, 30)      60          ['add_89[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_45 (Multi  (None, 81, 30)      2121        ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " add_90 (Add)                   (None, 81, 30)       0           ['multi_head_attention_45[0][0]',\n",
            "                                                                  'add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 81, 30)      60          ['add_90[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_45 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 81, 30)       0           ['sequential_45[0][0]',          \n",
            "                                                                  'add_90[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 81, 30)      60          ['add_91[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_46 (Multi  (None, 81, 30)      2121        ['layer_normalization_92[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 81, 30)       0           ['multi_head_attention_46[0][0]',\n",
            "                                                                  'add_91[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 81, 30)      60          ['add_92[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_46 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " add_93 (Add)                   (None, 81, 30)       0           ['sequential_46[0][0]',          \n",
            "                                                                  'add_92[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 81, 30)      60          ['add_93[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_47 (Multi  (None, 81, 30)      2121        ['layer_normalization_94[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " add_94 (Add)                   (None, 81, 30)       0           ['multi_head_attention_47[0][0]',\n",
            "                                                                  'add_93[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 81, 30)      60          ['add_94[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_47 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " add_95 (Add)                   (None, 81, 30)       0           ['sequential_47[0][0]',          \n",
            "                                                                  'add_94[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_96 (LayerN  (None, 81, 30)      60          ['add_95[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_48 (Multi  (None, 81, 30)      2121        ['layer_normalization_96[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " add_96 (Add)                   (None, 81, 30)       0           ['multi_head_attention_48[0][0]',\n",
            "                                                                  'add_95[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_97 (LayerN  (None, 81, 30)      60          ['add_96[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_48 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " add_97 (Add)                   (None, 81, 30)       0           ['sequential_48[0][0]',          \n",
            "                                                                  'add_96[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_98 (LayerN  (None, 81, 30)      60          ['add_97[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_49 (Multi  (None, 81, 30)      2121        ['layer_normalization_98[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " add_98 (Add)                   (None, 81, 30)       0           ['multi_head_attention_49[0][0]',\n",
            "                                                                  'add_97[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_99 (LayerN  (None, 81, 30)      60          ['add_98[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_49 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " add_99 (Add)                   (None, 81, 30)       0           ['sequential_49[0][0]',          \n",
            "                                                                  'add_98[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_100 (Layer  (None, 81, 30)      60          ['add_99[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_50 (Multi  (None, 81, 30)      2121        ['layer_normalization_100[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " add_100 (Add)                  (None, 81, 30)       0           ['multi_head_attention_50[0][0]',\n",
            "                                                                  'add_99[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_101 (Layer  (None, 81, 30)      60          ['add_100[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_50 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " add_101 (Add)                  (None, 81, 30)       0           ['sequential_50[0][0]',          \n",
            "                                                                  'add_100[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_102 (Layer  (None, 81, 30)      60          ['add_101[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_51 (Multi  (None, 81, 30)      2121        ['layer_normalization_102[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " add_102 (Add)                  (None, 81, 30)       0           ['multi_head_attention_51[0][0]',\n",
            "                                                                  'add_101[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_103 (Layer  (None, 81, 30)      60          ['add_102[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_51 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " add_103 (Add)                  (None, 81, 30)       0           ['sequential_51[0][0]',          \n",
            "                                                                  'add_102[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_104 (Layer  (None, 81, 30)      60          ['add_103[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_52 (Multi  (None, 81, 30)      2121        ['layer_normalization_104[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " add_104 (Add)                  (None, 81, 30)       0           ['multi_head_attention_52[0][0]',\n",
            "                                                                  'add_103[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_105 (Layer  (None, 81, 30)      60          ['add_104[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_52 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, 81, 30)       0           ['sequential_52[0][0]',          \n",
            "                                                                  'add_104[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 81, 30)      60          ['add_105[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_53 (Multi  (None, 81, 30)      2121        ['layer_normalization_106[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, 81, 30)       0           ['multi_head_attention_53[0][0]',\n",
            "                                                                  'add_105[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 81, 30)      60          ['add_106[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_53 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, 81, 30)       0           ['sequential_53[0][0]',          \n",
            "                                                                  'add_106[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_108 (Layer  (None, 81, 30)      60          ['add_107[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_54 (Multi  (None, 81, 30)      2121        ['layer_normalization_108[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, 81, 30)       0           ['multi_head_attention_54[0][0]',\n",
            "                                                                  'add_107[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_109 (Layer  (None, 81, 30)      60          ['add_108[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_54 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " add_109 (Add)                  (None, 81, 30)       0           ['sequential_54[0][0]',          \n",
            "                                                                  'add_108[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_110 (Layer  (None, 81, 30)      60          ['add_109[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_55 (Multi  (None, 81, 30)      2121        ['layer_normalization_110[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " add_110 (Add)                  (None, 81, 30)       0           ['multi_head_attention_55[0][0]',\n",
            "                                                                  'add_109[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_111 (Layer  (None, 81, 30)      60          ['add_110[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_55 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " add_111 (Add)                  (None, 81, 30)       0           ['sequential_55[0][0]',          \n",
            "                                                                  'add_110[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_112 (Layer  (None, 81, 30)      60          ['add_111[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_56 (Multi  (None, 81, 30)      2121        ['layer_normalization_112[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " add_112 (Add)                  (None, 81, 30)       0           ['multi_head_attention_56[0][0]',\n",
            "                                                                  'add_111[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_113 (Layer  (None, 81, 30)      60          ['add_112[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_56 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " add_113 (Add)                  (None, 81, 30)       0           ['sequential_56[0][0]',          \n",
            "                                                                  'add_112[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_114 (Layer  (None, 81, 30)      60          ['add_113[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_57 (Multi  (None, 81, 30)      2121        ['layer_normalization_114[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " add_114 (Add)                  (None, 81, 30)       0           ['multi_head_attention_57[0][0]',\n",
            "                                                                  'add_113[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_115 (Layer  (None, 81, 30)      60          ['add_114[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_57 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " add_115 (Add)                  (None, 81, 30)       0           ['sequential_57[0][0]',          \n",
            "                                                                  'add_114[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_116 (Layer  (None, 81, 30)      60          ['add_115[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_58 (Multi  (None, 81, 30)      2121        ['layer_normalization_116[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " add_116 (Add)                  (None, 81, 30)       0           ['multi_head_attention_58[0][0]',\n",
            "                                                                  'add_115[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_117 (Layer  (None, 81, 30)      60          ['add_116[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_58 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " add_117 (Add)                  (None, 81, 30)       0           ['sequential_58[0][0]',          \n",
            "                                                                  'add_116[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_118 (Layer  (None, 81, 30)      60          ['add_117[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_59 (Multi  (None, 81, 30)      2121        ['layer_normalization_118[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " add_118 (Add)                  (None, 81, 30)       0           ['multi_head_attention_59[0][0]',\n",
            "                                                                  'add_117[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_119 (Layer  (None, 81, 30)      60          ['add_118[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_59 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " add_119 (Add)                  (None, 81, 30)       0           ['sequential_59[0][0]',          \n",
            "                                                                  'add_118[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_120 (Layer  (None, 81, 30)      60          ['add_119[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_60 (Multi  (None, 81, 30)      2121        ['layer_normalization_120[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " add_120 (Add)                  (None, 81, 30)       0           ['multi_head_attention_60[0][0]',\n",
            "                                                                  'add_119[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_121 (Layer  (None, 81, 30)      60          ['add_120[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_60 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " add_121 (Add)                  (None, 81, 30)       0           ['sequential_60[0][0]',          \n",
            "                                                                  'add_120[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_122 (Layer  (None, 81, 30)      60          ['add_121[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_61 (Multi  (None, 81, 30)      2121        ['layer_normalization_122[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " add_122 (Add)                  (None, 81, 30)       0           ['multi_head_attention_61[0][0]',\n",
            "                                                                  'add_121[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_123 (Layer  (None, 81, 30)      60          ['add_122[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " sequential_61 (Sequential)     (None, 81, 30)       7350        ['layer_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " add_123 (Add)                  (None, 81, 30)       0           ['sequential_61[0][0]',          \n",
            "                                                                  'add_122[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_124 (Layer  (None, 81, 30)      60          ['add_123[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 30)          0           ['layer_normalization_124[0][0]']\n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_124 (Dense)              (None, 2)            62          ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 627,944\n",
            "Trainable params: 627,944\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/ucf_of_pretrained_on_lad/checkpoint.tf/'\n",
        "model_checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )"
      ],
      "metadata": {
        "id": "YtRHscscND2J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/ucf_of_pretrained_on_lad/checkpoint.tf/'\n",
        "checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "                     monitor=\"val_loss\",\n",
        "                     min_delta=0.025,\n",
        "                     patience=5,\n",
        "                     verbose=0,\n",
        "                     mode=\"min\",\n",
        "                     baseline=None,\n",
        "                     restore_best_weights=False\n",
        "                 )\n",
        "\n",
        "values = np.linspace(0.00001,LEARNING_RATE,14)[::-1].astype(np.float32)\n",
        "boundaries = np.linspace(5, 45,13)[:values.shape[0]-1].astype(np.int32)\n",
        "\n",
        "scheduler = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    list(boundaries), list(values))\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=3, min_lr=0.000001)\n",
        "\n",
        "callbacks = [checkpoint, lr_scheduler, reduce_lr]"
      ],
      "metadata": {
        "id": "K4C-FTdXNWCg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    new_model = model\n",
        "    opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    new_model.compile(loss=loss_fn,optimizer=opt,metrics=['accuracy'])\n",
        "    #model.load_weights(checkpoint_filepath)\n",
        "    # Train the model.\n",
        "    with tf.device('/GPU:0'):\n",
        "      history = new_model.fit(training_generator, epochs=5, validation_data=(validation_generator), callbacks=[model_checkpoint, callbacks])\n",
        "\n",
        "    return new_model, history\n",
        "\n",
        "\n",
        "model, history = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8LljGphMLaw",
        "outputId": "99e3642e-2c0d-4b38-9389-682cbe127f69"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/5\n",
            "2286/2286 [==============================] - 1102s 391ms/step - loss: 0.5579 - accuracy: 0.7098 - val_loss: 0.3518 - val_accuracy: 0.8425 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/5\n",
            "2286/2286 [==============================] - 765s 335ms/step - loss: 0.4102 - accuracy: 0.8084 - val_loss: 0.3603 - val_accuracy: 0.8440 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/5\n",
            "2286/2286 [==============================] - 780s 341ms/step - loss: 0.3479 - accuracy: 0.8478 - val_loss: 0.3202 - val_accuracy: 0.8578 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 4/5\n",
            "2286/2286 [==============================] - 764s 334ms/step - loss: 0.3111 - accuracy: 0.8666 - val_loss: 0.3702 - val_accuracy: 0.8532 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 5/5\n",
            "2286/2286 [==============================] - 768s 336ms/step - loss: 0.2940 - accuracy: 0.8766 - val_loss: 0.3578 - val_accuracy: 0.8364 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "voSmkewE1ZcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "def eval_model(model,x,y):\n",
        "#     print(model.evaluate(x.reshape(x.shape+(1,)),y_encoded))\n",
        "    y_pred = model.predict(x)\n",
        "    y_pred = np.argmax(y_pred,axis = -1)\n",
        "    y_numbers = np.argmax(y,axis=-1)\n",
        "\n",
        "    target_names = ['normal','violence']\n",
        "    labels = target_names\n",
        "    tick_marks = np.arange(len(labels))\n",
        "\n",
        "    print(classification_report(y_numbers, y_pred, target_names=target_names))\n",
        "\n",
        "    z = tf.math.confusion_matrix(y_numbers,y_pred).numpy().astype(np.int64)\n",
        "    z = np.around(z.astype('float') / z.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "\n",
        "    x = target_names\n",
        "    y = target_names\n",
        "\n",
        "    # change each element of z to type string for annotations\n",
        "    z_text = [[str(y) for y in x] for x in z]\n",
        "\n",
        "    # set up figure\n",
        "    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
        "\n",
        "    # add title\n",
        "    fig.update_layout(title_text='<i><b>Confusion matrix</b></i>',\n",
        "                    #xaxis = dict(title='x'),\n",
        "                    #yaxis = dict(title='x')\n",
        "                    )\n",
        "\n",
        "    # add custom xaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=0.5,\n",
        "                            y=-0.15,\n",
        "                            showarrow=False,\n",
        "                            text=\"Predicted value\",\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # add custom yaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=-0.35,\n",
        "                            y=0.5,\n",
        "                            showarrow=False,\n",
        "                            text=\"Real value\",\n",
        "                            textangle=-90,\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # adjust margins to make room for yaxis title\n",
        "    fig.update_layout(margin=dict(t=50, l=200))\n",
        "\n",
        "    # add colorbar\n",
        "    fig['data'][0]['showscale'] = True\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "4SJywpkb7PC0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note: you may replicate the next cell to test on the train, validation or test dataset"
      ],
      "metadata": {
        "id": "fzPT2hT62LIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing on LAd2000"
      ],
      "metadata": {
        "id": "EKktG9Yj578M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, X_test_ucf, y_test_ucf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "ylEBMmz07XO2",
        "outputId": "b5496056-30c6-41ae-8a05-48529d5861d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 16s 132ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.79      0.72      0.76       682\n",
            "    violence       0.74      0.81      0.77       663\n",
            "\n",
            "    accuracy                           0.76      1345\n",
            "   macro avg       0.77      0.76      0.76      1345\n",
            "weighted avg       0.77      0.76      0.76      1345\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"8a0a9277-0652-4632-a3bd-499970574765\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8a0a9277-0652-4632-a3bd-499970574765\")) {                    Plotly.newPlot(                        \"8a0a9277-0652-4632-a3bd-499970574765\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"reversescale\":false,\"showscale\":true,\"x\":[\"normal\",\"violence\"],\"y\":[\"normal\",\"violence\"],\"z\":[[0.724,0.276],[0.195,0.805]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"0.724\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.276\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.195\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"0.805\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Predicted value\",\"x\":0.5,\"xref\":\"paper\",\"y\":-0.15,\"yref\":\"paper\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Real value\",\"textangle\":-90,\"x\":-0.35,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\"},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"<i><b>Confusion matrix</b></i>\"},\"margin\":{\"t\":50,\"l\":200}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8a0a9277-0652-4632-a3bd-499970574765');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_ucf, y_test_ucf)"
      ],
      "metadata": {
        "id": "r3Ed-OUE2Frw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6f72a3-1a3f-4558-90ca-a94bb362d74b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 7s 149ms/step - loss: 0.5305 - accuracy: 0.7643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5305256843566895, 0.7643122673034668]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix as cm\n",
        "\n",
        "def eval_model_f(model,x,y):\n",
        "#     print(model.evaluate(x.reshape(x.shape+(1,)),y_encoded))\n",
        "    y_pred = model.predict(x)\n",
        "    y_pred = np.argmax(y_pred,axis = -1)\n",
        "    y_numbers = np.argmax(y,axis=-1)\n",
        "\n",
        "    target_names = [\n",
        "  'normal', 'violence'\n",
        "    ]\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    print(classification_report(y_numbers, y_pred, target_names=target_names))\n",
        "\n",
        "    conf = cm(y_numbers,y_pred)\n",
        "\n",
        "    sns.heatmap(conf,annot=True)\n",
        "    plt.xticks(tick_marks,target_names,rotation=45)\n",
        "    plt.yticks(tick_marks,target_names,rotation=0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Qyiito7AWq5M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_f(model,X_test_ucf,y_test_ucf)"
      ],
      "metadata": {
        "id": "d9Y56X8nWqxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "b1209711-6fff-456b-dbc5-7a7aa60b79e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 6s 145ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.79      0.72      0.76       682\n",
            "    violence       0.74      0.81      0.77       663\n",
            "\n",
            "    accuracy                           0.76      1345\n",
            "   macro avg       0.77      0.76      0.76      1345\n",
            "weighted avg       0.77      0.76      0.76      1345\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}