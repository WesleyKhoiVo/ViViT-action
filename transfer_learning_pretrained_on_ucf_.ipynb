{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Note: Check paths before running\n",
        "\n"
      ],
      "metadata": {
        "id": "-kIPzYrYxEis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "FmPADVV4uv2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import ipywidgets\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils import to_categorical, OrderedEnqueuer\n",
        "\n",
        "import ipywidgets\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback, WandbMetricsLogger\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "sbvx0xxEuvXi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drive mounting"
      ],
      "metadata": {
        "id": "tgDV5Dsau2jI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kgJrQtAueAi",
        "outputId": "e61d5744-70ab-49e7-8592-88115529e5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters( after hypertuning on UCF)"
      ],
      "metadata": {
        "id": "VE8Z4kWN29w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (75, 75, 15, 3)\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = 10\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 510\n",
        "NUM_HEADS = 17\n",
        "NUM_LAYERS = 9"
      ],
      "metadata": {
        "id": "VPCXXWYPqr2M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Generator"
      ],
      "metadata": {
        "id": "c1F8slLE1zCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_in, y_in, batch_size=BATCH_SIZE, shuffle=True):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.x = x_in\n",
        "        self.y = y_in\n",
        "        self.datalen = len(y_in)\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get batch indexes from shuffled indexes\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x_batch = self.x[batch_indexes]\n",
        "        y_batch = self.y[batch_indexes]\n",
        "        return x_batch, y_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch\n",
        "        return self.datalen // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updates indexes after each epoch\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "uImgcpBQ1xLA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross testing the trained model"
      ],
      "metadata": {
        "id": "Pfk8ppEbvICg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Data"
      ],
      "metadata": {
        "id": "HHR0pjbEvDBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH_LAD2000 = '/content/gdrive/MyDrive/All extracted frames/LAD2000_Vio_Nor/Vio_nor/'"
      ],
      "metadata": {
        "id": "0_pnWJ4ov-Wo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LAD2000"
      ],
      "metadata": {
        "id": "IAxr0hH_xfuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lad, y_train_lad = np.load(DATA_PATH_LAD2000+ 'Train/X_train.npy',mmap_mode='r'), np.load(DATA_PATH_LAD2000 + 'Train/y_train.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "HyErOfAswqKC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_lad, y_test_lad = np.load(DATA_PATH_LAD2000+ 'Test/X_Test.npy',mmap_mode='r'), np.load(DATA_PATH_LAD2000+ 'Test/y_test.npy',mmap_mode='r')"
      ],
      "metadata": {
        "id": "uZkIwLOuxa97"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lad, y_val_lad = np.load(DATA_PATH_LAD2000 + 'Val/X_val.npy',mmap_mode='r'), np.load(DATA_PATH_LAD2000 + 'Val/y_val.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "0QJEaT3gxa7Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lad.shape, y_train_lad.shape"
      ],
      "metadata": {
        "id": "LZAYi87Txayk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f373bda5-2143-4b8d-9c29-afbb2edbf166"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4778, 15, 75, 75, 3), (4778,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_lad.shape, y_test_lad.shape"
      ],
      "metadata": {
        "id": "wQrlTfstxarv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02a4328-bfbd-446e-85b7-8fd73b081d7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1186, 15, 75, 75, 3), (1186,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lad.shape, y_val_lad.shape"
      ],
      "metadata": {
        "id": "TO8oH0u1xwao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7d15cd-0112-446d-bc5a-cd3ebe603fb1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1149, 15, 75, 75, 3), (1149,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Labels"
      ],
      "metadata": {
        "id": "ZuNCTOc61vpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_lad = to_categorical(y_train_lad, 2)\n",
        "y_val_lad = to_categorical(y_val_lad, 2)\n",
        "y_test_lad = to_categorical(y_test_lad, 2)"
      ],
      "metadata": {
        "id": "vtcJPoRZ1xSS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lad = np.moveaxis(X_train_lad, 1, 3)\n",
        "X_val_lad = np.moveaxis(X_val_lad, 1, 3)\n",
        "X_test_lad = np.moveaxis(X_test_lad, 1, 3)"
      ],
      "metadata": {
        "id": "CGf5ESgvNhcl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = DataGenerator(X_train_lad, y_train_lad)\n",
        "validation_generator = DataGenerator(X_val_lad, y_val_lad)"
      ],
      "metadata": {
        "id": "v8uDkglHOAsw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ViVit Model\n"
      ],
      "metadata": {
        "id": "PPKHIq1G2Y8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ],
      "metadata": {
        "id": "gpL6HxWM2YZ2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "6nzu1V2O2hD8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    transformer_layers,\n",
        "    num_heads,\n",
        "    embed_dim,\n",
        "    dropout,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=2,\n",
        "):\n",
        "    input_shape = INPUT_SHAPE\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "R_mQLRT_2jlp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the Model"
      ],
      "metadata": {
        "id": "jQxSEmqox0K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH = '/content/gdrive/MyDrive/Checkpoints (1)/hptuning-norm-ucf-vivit(best)/checkpoint.tf'"
      ],
      "metadata": {
        "id": "QKHAQQw1vHbg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "                embed_dim=121, patch_size=(\n",
        "                    3,3,3)\n",
        "            ),\n",
        "            positional_encoder=PositionalEncoder(embed_dim=121),\n",
        "        transformer_layers=7,\n",
        "        num_heads=2,\n",
        "        embed_dim=121,\n",
        "        dropout=0.3,\n",
        "    )\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ol2HC0jEuILW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(MODEL_PATH)"
      ],
      "metadata": {
        "id": "NebVaS-w1T08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e00f64-256e-4f77-bc2c-dc197cc12c07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7efc322f6c80>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGS5KV9U0-Xr",
        "outputId": "cd4a63c2-88f9-432a-a3d7-84348e331c2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 75, 75, 15,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding (TubeletEmbe  (None, 3125, 121)   9922        ['input_1[0][0]']                \n",
            " dding)                                                                                           \n",
            "                                                                                                  \n",
            " positional_encoder (Positional  (None, 3125, 121)   378125      ['tubelet_embedding[0][0]']      \n",
            " Encoder)                                                                                         \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 3125, 121)   242         ['positional_encoder[0][0]']     \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 3125, 121)   58561       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 3125, 121)    0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'positional_encoder[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 3125, 121)   242         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 3125, 121)    117733      ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 3125, 121)    0           ['sequential[0][0]',             \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 3125, 121)   242         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 3125, 121)    0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 3125, 121)   242         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 3125, 121)    0           ['sequential_1[0][0]',           \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 3125, 121)   242         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 3125, 121)    0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 3125, 121)   242         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 3125, 121)    0           ['sequential_2[0][0]',           \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 3125, 121)   242         ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 3125, 121)    0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 3125, 121)   242         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 3125, 121)    0           ['sequential_3[0][0]',           \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 3125, 121)   242         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 3125, 121)    0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 3125, 121)   242         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 3125, 121)    0           ['sequential_4[0][0]',           \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 3125, 121)   242         ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 3125, 121)    0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 3125, 121)   242         ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 3125, 121)    0           ['sequential_5[0][0]',           \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 3125, 121)   242         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 3125, 121)   58561       ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 3125, 121)    0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 3125, 121)   242         ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_6 (Sequential)      (None, 3125, 121)    117733      ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 3125, 121)    0           ['sequential_6[0][0]',           \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 3125, 121)   242         ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 121)         0           ['layer_normalization_14[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 2)            244         ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,625,979\n",
            "Trainable params: 1,625,979\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/lad_pretrained_on_ucf_binary/checkpoint/'\n",
        "model_checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )"
      ],
      "metadata": {
        "id": "YtRHscscND2J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/lad_pretrained_on_ucf_binary/checkpoint/'\n",
        "checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "                     monitor=\"val_loss\",\n",
        "                     min_delta=0.025,\n",
        "                     patience=5,\n",
        "                     verbose=0,\n",
        "                     mode=\"min\",\n",
        "                     baseline=None,\n",
        "                     restore_best_weights=False\n",
        "                 )\n",
        "\n",
        "values = np.linspace(0.00001,LEARNING_RATE,14)[::-1].astype(np.float32)\n",
        "boundaries = np.linspace(5, 45,13)[:values.shape[0]-1].astype(np.int32)\n",
        "\n",
        "scheduler = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    list(boundaries), list(values))\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=3, min_lr=0.000001)\n",
        "\n",
        "callbacks = [checkpoint, lr_scheduler, reduce_lr]"
      ],
      "metadata": {
        "id": "K4C-FTdXNWCg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    model.load_weights(MODEL_PATH)\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    new_model = model\n",
        "    opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    new_model.compile(loss=loss_fn,optimizer=opt,metrics=['accuracy'])\n",
        "    #model.load_weights(checkpoint_filepath)\n",
        "    # Train the model.\n",
        "    history = new_model.fit(training_generator, epochs=10, validation_data=(validation_generator), callbacks=[model_checkpoint, callbacks])\n",
        "\n",
        "    return new_model, history\n",
        "\n",
        "\n",
        "model, history = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8LljGphMLaw",
        "outputId": "ac919f45-50cf-4722-a8cc-292138070607"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "1194/1194 [==============================] - 680s 546ms/step - loss: 0.7243 - accuracy: 0.5272 - val_loss: 0.7085 - val_accuracy: 0.4774 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "1194/1194 [==============================] - 649s 544ms/step - loss: 0.6794 - accuracy: 0.5691 - val_loss: 0.7434 - val_accuracy: 0.4834 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "1194/1194 [==============================] - 684s 573ms/step - loss: 0.6483 - accuracy: 0.5951 - val_loss: 0.7290 - val_accuracy: 0.5523 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 4/10\n",
            "1194/1194 [==============================] - 649s 544ms/step - loss: 0.6187 - accuracy: 0.6154 - val_loss: 0.7776 - val_accuracy: 0.5235 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 5/10\n",
            "1194/1194 [==============================] - 650s 544ms/step - loss: 0.6042 - accuracy: 0.6367 - val_loss: 0.7828 - val_accuracy: 0.4861 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 6/10\n",
            "1194/1194 [==============================] - 684s 573ms/step - loss: 0.5970 - accuracy: 0.6317 - val_loss: 0.8143 - val_accuracy: 0.4878 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 7/10\n",
            "1194/1194 [==============================] - 650s 544ms/step - loss: 0.5879 - accuracy: 0.6378 - val_loss: 0.8135 - val_accuracy: 0.5279 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 8/10\n",
            "1194/1194 [==============================] - 650s 544ms/step - loss: 0.5728 - accuracy: 0.6505 - val_loss: 0.8072 - val_accuracy: 0.5845 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 9/10\n",
            "1194/1194 [==============================] - 651s 545ms/step - loss: 0.5721 - accuracy: 0.6612 - val_loss: 0.8726 - val_accuracy: 0.4878 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 8.615384285803884e-05.\n",
            "Epoch 10/10\n",
            "1194/1194 [==============================] - 651s 546ms/step - loss: 0.5456 - accuracy: 0.6813 - val_loss: 0.9955 - val_accuracy: 0.5348 - lr: 8.6154e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "voSmkewE1ZcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "def eval_model(model,x,y):\n",
        "#     print(model.evaluate(x.reshape(x.shape+(1,)),y_encoded))\n",
        "    y_pred = model.predict(x)\n",
        "    y_pred = np.argmax(y_pred,axis = -1)\n",
        "    y_numbers = np.argmax(y,axis=-1)\n",
        "\n",
        "    target_names = ['normal','violence']\n",
        "    labels = target_names\n",
        "    tick_marks = np.arange(len(labels))\n",
        "\n",
        "    print(classification_report(y_numbers, y_pred, target_names=target_names))\n",
        "\n",
        "    z = tf.math.confusion_matrix(y_numbers,y_pred).numpy().astype(np.int64)\n",
        "    z = np.around(z.astype('float') / z.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "\n",
        "    x = target_names\n",
        "    y = target_names\n",
        "\n",
        "    # change each element of z to type string for annotations\n",
        "    z_text = [[str(y) for y in x] for x in z]\n",
        "\n",
        "    # set up figure\n",
        "    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
        "\n",
        "    # add title\n",
        "    fig.update_layout(title_text='<i><b>Confusion matrix</b></i>',\n",
        "                    #xaxis = dict(title='x'),\n",
        "                    #yaxis = dict(title='x')\n",
        "                    )\n",
        "\n",
        "    # add custom xaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=0.5,\n",
        "                            y=-0.15,\n",
        "                            showarrow=False,\n",
        "                            text=\"Predicted value\",\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # add custom yaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=-0.35,\n",
        "                            y=0.5,\n",
        "                            showarrow=False,\n",
        "                            text=\"Real value\",\n",
        "                            textangle=-90,\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # adjust margins to make room for yaxis title\n",
        "    fig.update_layout(margin=dict(t=50, l=200))\n",
        "\n",
        "    # add colorbar\n",
        "    fig['data'][0]['showscale'] = True\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "4SJywpkb7PC0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note: you may replicate the next cell to test on the train, validation or test dataset"
      ],
      "metadata": {
        "id": "fzPT2hT62LIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing on LAd2000"
      ],
      "metadata": {
        "id": "EKktG9Yj578M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator_LAD = DataGenerator(X_test_lad, y_test_lad)  #whether it's ucf or lad"
      ],
      "metadata": {
        "id": "iyT163Kf1Y1L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, X_test_lad, y_test_lad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "ylEBMmz07XO2",
        "outputId": "b235baba-00f8-4fcb-94f4-2c8dc8c2c205"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 52s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.52      0.75      0.61       570\n",
            "    violence       0.60      0.35      0.45       616\n",
            "\n",
            "    accuracy                           0.54      1186\n",
            "   macro avg       0.56      0.55      0.53      1186\n",
            "weighted avg       0.56      0.54      0.52      1186\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"2b14b46d-c1fc-4aee-b423-08c62359c352\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2b14b46d-c1fc-4aee-b423-08c62359c352\")) {                    Plotly.newPlot(                        \"2b14b46d-c1fc-4aee-b423-08c62359c352\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"reversescale\":false,\"showscale\":true,\"x\":[\"normal\",\"violence\"],\"y\":[\"normal\",\"violence\"],\"z\":[[0.746,0.254],[0.646,0.354]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"0.746\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.254\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"0.646\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.354\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Predicted value\",\"x\":0.5,\"xref\":\"paper\",\"y\":-0.15,\"yref\":\"paper\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Real value\",\"textangle\":-90,\"x\":-0.35,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\"},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"<i><b>Confusion matrix</b></i>\"},\"margin\":{\"t\":50,\"l\":200}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2b14b46d-c1fc-4aee-b423-08c62359c352');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator_LAD)"
      ],
      "metadata": {
        "id": "r3Ed-OUE2Frw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84242a99-8e7e-4486-fca5-e0a840e0b6a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "296/296 [==============================] - 50s 170ms/step - loss: 0.7520 - accuracy: 0.5414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7519758939743042, 0.5413851141929626]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}