{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Note: Check paths before running\n",
        "\n"
      ],
      "metadata": {
        "id": "-kIPzYrYxEis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "FmPADVV4uv2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import ipywidgets\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils import to_categorical, OrderedEnqueuer\n",
        "\n",
        "import ipywidgets\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback, WandbMetricsLogger\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "sbvx0xxEuvXi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drive mounting"
      ],
      "metadata": {
        "id": "tgDV5Dsau2jI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kgJrQtAueAi",
        "outputId": "72e4a47c-e978-493f-f7de-04db573c6673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters( after hypertuning on UCF)"
      ],
      "metadata": {
        "id": "VE8Z4kWN29w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (75, 75, 15, 3)\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = 10\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 510\n",
        "NUM_HEADS = 17\n",
        "NUM_LAYERS = 9"
      ],
      "metadata": {
        "id": "VPCXXWYPqr2M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Generator"
      ],
      "metadata": {
        "id": "c1F8slLE1zCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_in, y_in, batch_size=BATCH_SIZE, shuffle=True):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.x = x_in\n",
        "        self.y = y_in\n",
        "        self.datalen = len(y_in)\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # get batch indexes from shuffled indexes\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x_batch = self.x[batch_indexes]\n",
        "        y_batch = self.y[batch_indexes]\n",
        "        return x_batch, y_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch\n",
        "        return self.datalen // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Updates indexes after each epoch\n",
        "        self.indexes = np.arange(self.datalen)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "uImgcpBQ1xLA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross testing the trained model"
      ],
      "metadata": {
        "id": "Pfk8ppEbvICg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Data"
      ],
      "metadata": {
        "id": "HHR0pjbEvDBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH_UCF = '/content/gdrive/MyDrive/Datasets/UCF-original/Binary_Classification/Vio_Nor_Binary_data/'"
      ],
      "metadata": {
        "id": "0_pnWJ4ov-Wo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LAD2000"
      ],
      "metadata": {
        "id": "IAxr0hH_xfuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ucf, y_train_ucf = np.load(DATA_PATH_UCF+ 'Train/X_train.npy',mmap_mode='r'), np.load(DATA_PATH_UCF + 'Train/y_train.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "HyErOfAswqKC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf, y_test_ucf = np.load(DATA_PATH_UCF+ 'Test/X_test.npy',mmap_mode='r'), np.load(DATA_PATH_UCF+ 'Test/y_test.npy',mmap_mode='r')"
      ],
      "metadata": {
        "id": "uZkIwLOuxa97"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ucf, y_val_ucf = np.load(DATA_PATH_UCF + 'Val/X_val.npy',mmap_mode='r'), np.load(DATA_PATH_UCF + 'Val/y_val.npy', mmap_mode='r')"
      ],
      "metadata": {
        "id": "0QJEaT3gxa7Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ucf.shape, y_train_ucf.shape"
      ],
      "metadata": {
        "id": "LZAYi87Txayk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a258288-19c7-4da3-fc1a-5d8779738fc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4573, 15, 75, 75, 3), (4573,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf.shape, y_test_ucf.shape"
      ],
      "metadata": {
        "id": "wQrlTfstxarv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba169eb-2873-49f2-a702-fdee8166fdd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1345, 15, 75, 75, 3), (1345,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_ucf.shape, y_val_ucf.shape"
      ],
      "metadata": {
        "id": "TO8oH0u1xwao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba33b98-8e6c-4c75-b889-a801039cc080"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((655, 15, 75, 75, 3), (655,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Labels"
      ],
      "metadata": {
        "id": "ZuNCTOc61vpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_ucf = to_categorical(y_train_ucf, 2)\n",
        "y_val_ucf = to_categorical(y_val_ucf, 2)\n",
        "y_test_ucf = to_categorical(y_test_ucf, 2)"
      ],
      "metadata": {
        "id": "vtcJPoRZ1xSS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ucf = np.moveaxis(X_train_ucf, 1, 3)\n",
        "X_val_ucf = np.moveaxis(X_val_ucf, 1, 3)\n",
        "X_test_ucf = np.moveaxis(X_test_ucf, 1, 3)"
      ],
      "metadata": {
        "id": "CGf5ESgvNhcl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = DataGenerator(X_train_ucf, y_train_ucf)\n",
        "validation_generator = DataGenerator(X_val_ucf, y_val_ucf)"
      ],
      "metadata": {
        "id": "v8uDkglHOAsw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ViVit Model\n"
      ],
      "metadata": {
        "id": "PPKHIq1G2Y8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ],
      "metadata": {
        "id": "gpL6HxWM2YZ2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "6nzu1V2O2hD8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    transformer_layers,\n",
        "    num_heads,\n",
        "    embed_dim,\n",
        "    dropout,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=2,\n",
        "):\n",
        "    input_shape = INPUT_SHAPE\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=dropout\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "R_mQLRT_2jlp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the Model"
      ],
      "metadata": {
        "id": "jQxSEmqox0K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_PATH = '/content/gdrive/MyDrive/Checkpoints (1)/hptuning-norm-lad-vivit/checkpoint.tf'"
      ],
      "metadata": {
        "id": "QKHAQQw1vHbg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "                embed_dim=121, patch_size=(\n",
        "                    3,3,3)\n",
        "            ),\n",
        "            positional_encoder=PositionalEncoder(embed_dim=121),\n",
        "        transformer_layers=7,\n",
        "        num_heads=2,\n",
        "        embed_dim=121,\n",
        "        dropout=0.3,\n",
        "    )\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ol2HC0jEuILW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(MODEL_PATH)"
      ],
      "metadata": {
        "id": "NebVaS-w1T08"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGS5KV9U0-Xr",
        "outputId": "a70b4fbf-69c1-4089-fa1c-f9b5c671dee7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 15, 75, 75,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " tubelet_embedding (TubeletEmbe  (None, 36, 41)      212585      ['input_1[0][0]']                \n",
            " dding)                                                                                           \n",
            "                                                                                                  \n",
            " positional_encoder (Positional  (None, 36, 41)      1476        ['tubelet_embedding[0][0]']      \n",
            " Encoder)                                                                                         \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 36, 41)      82          ['positional_encoder[0][0]']     \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 36, 41)      6053        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 36, 41)       0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'positional_encoder[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 36, 41)      82          ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 36, 41)       13653       ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 36, 41)       0           ['sequential[0][0]',             \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 36, 41)      82          ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 36, 41)      6053        ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 36, 41)       0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 36, 41)      82          ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 36, 41)       0           ['sequential_1[0][0]',           \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 36, 41)      82          ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 36, 41)      6053        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 36, 41)       0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 36, 41)      82          ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 36, 41)       0           ['sequential_2[0][0]',           \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 36, 41)      82          ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 36, 41)      6053        ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 36, 41)       0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 36, 41)      82          ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 36, 41)       0           ['sequential_3[0][0]',           \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 36, 41)      82          ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 36, 41)      6053        ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 36, 41)       0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 36, 41)      82          ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 36, 41)       0           ['sequential_4[0][0]',           \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 36, 41)      82          ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 36, 41)      6053        ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 36, 41)       0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 36, 41)      82          ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_5 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 36, 41)       0           ['sequential_5[0][0]',           \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 36, 41)      82          ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 36, 41)      6053        ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 36, 41)       0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 36, 41)      82          ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_6 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 36, 41)       0           ['sequential_6[0][0]',           \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 36, 41)      82          ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 36, 41)      6053        ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 36, 41)       0           ['multi_head_attention_7[0][0]', \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 36, 41)      82          ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_7 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 36, 41)       0           ['sequential_7[0][0]',           \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 36, 41)      82          ['add_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 36, 41)      6053        ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 36, 41)       0           ['multi_head_attention_8[0][0]', \n",
            "                                                                  'add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 36, 41)      82          ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_8 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 36, 41)       0           ['sequential_8[0][0]',           \n",
            "                                                                  'add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 36, 41)      82          ['add_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 36, 41)      6053        ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 36, 41)       0           ['multi_head_attention_9[0][0]', \n",
            "                                                                  'add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 36, 41)      82          ['add_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_9 (Sequential)      (None, 36, 41)       13653       ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 36, 41)       0           ['sequential_9[0][0]',           \n",
            "                                                                  'add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 36, 41)      82          ['add_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 36, 41)      6053        ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 36, 41)       0           ['multi_head_attention_10[0][0]',\n",
            "                                                                  'add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 36, 41)      82          ['add_20[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_10 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 36, 41)       0           ['sequential_10[0][0]',          \n",
            "                                                                  'add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 36, 41)      82          ['add_21[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 36, 41)      6053        ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 36, 41)       0           ['multi_head_attention_11[0][0]',\n",
            "                                                                  'add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 36, 41)      82          ['add_22[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_11 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 36, 41)       0           ['sequential_11[0][0]',          \n",
            "                                                                  'add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 36, 41)      82          ['add_23[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 36, 41)      6053        ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 36, 41)       0           ['multi_head_attention_12[0][0]',\n",
            "                                                                  'add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 36, 41)      82          ['add_24[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_12 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 36, 41)       0           ['sequential_12[0][0]',          \n",
            "                                                                  'add_24[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 36, 41)      82          ['add_25[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 36, 41)      6053        ['layer_normalization_26[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 36, 41)       0           ['multi_head_attention_13[0][0]',\n",
            "                                                                  'add_25[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 36, 41)      82          ['add_26[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_13 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 36, 41)       0           ['sequential_13[0][0]',          \n",
            "                                                                  'add_26[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 36, 41)      82          ['add_27[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 36, 41)      6053        ['layer_normalization_28[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 36, 41)       0           ['multi_head_attention_14[0][0]',\n",
            "                                                                  'add_27[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 36, 41)      82          ['add_28[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_14 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 36, 41)       0           ['sequential_14[0][0]',          \n",
            "                                                                  'add_28[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 36, 41)      82          ['add_29[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 36, 41)      6053        ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 36, 41)       0           ['multi_head_attention_15[0][0]',\n",
            "                                                                  'add_29[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 36, 41)      82          ['add_30[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_15 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 36, 41)       0           ['sequential_15[0][0]',          \n",
            "                                                                  'add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 36, 41)      82          ['add_31[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (Multi  (None, 36, 41)      6053        ['layer_normalization_32[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 36, 41)       0           ['multi_head_attention_16[0][0]',\n",
            "                                                                  'add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 36, 41)      82          ['add_32[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_16 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 36, 41)       0           ['sequential_16[0][0]',          \n",
            "                                                                  'add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 36, 41)      82          ['add_33[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (Multi  (None, 36, 41)      6053        ['layer_normalization_34[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 36, 41)       0           ['multi_head_attention_17[0][0]',\n",
            "                                                                  'add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 36, 41)      82          ['add_34[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_17 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 36, 41)       0           ['sequential_17[0][0]',          \n",
            "                                                                  'add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_36 (LayerN  (None, 36, 41)      82          ['add_35[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (Multi  (None, 36, 41)      6053        ['layer_normalization_36[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " add_36 (Add)                   (None, 36, 41)       0           ['multi_head_attention_18[0][0]',\n",
            "                                                                  'add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_37 (LayerN  (None, 36, 41)      82          ['add_36[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_18 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " add_37 (Add)                   (None, 36, 41)       0           ['sequential_18[0][0]',          \n",
            "                                                                  'add_36[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_38 (LayerN  (None, 36, 41)      82          ['add_37[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (Multi  (None, 36, 41)      6053        ['layer_normalization_38[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " add_38 (Add)                   (None, 36, 41)       0           ['multi_head_attention_19[0][0]',\n",
            "                                                                  'add_37[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_39 (LayerN  (None, 36, 41)      82          ['add_38[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_19 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " add_39 (Add)                   (None, 36, 41)       0           ['sequential_19[0][0]',          \n",
            "                                                                  'add_38[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_40 (LayerN  (None, 36, 41)      82          ['add_39[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (Multi  (None, 36, 41)      6053        ['layer_normalization_40[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " add_40 (Add)                   (None, 36, 41)       0           ['multi_head_attention_20[0][0]',\n",
            "                                                                  'add_39[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_41 (LayerN  (None, 36, 41)      82          ['add_40[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_20 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " add_41 (Add)                   (None, 36, 41)       0           ['sequential_20[0][0]',          \n",
            "                                                                  'add_40[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_42 (LayerN  (None, 36, 41)      82          ['add_41[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (Multi  (None, 36, 41)      6053        ['layer_normalization_42[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " add_42 (Add)                   (None, 36, 41)       0           ['multi_head_attention_21[0][0]',\n",
            "                                                                  'add_41[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_43 (LayerN  (None, 36, 41)      82          ['add_42[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_21 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " add_43 (Add)                   (None, 36, 41)       0           ['sequential_21[0][0]',          \n",
            "                                                                  'add_42[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_44 (LayerN  (None, 36, 41)      82          ['add_43[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (Multi  (None, 36, 41)      6053        ['layer_normalization_44[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " add_44 (Add)                   (None, 36, 41)       0           ['multi_head_attention_22[0][0]',\n",
            "                                                                  'add_43[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_45 (LayerN  (None, 36, 41)      82          ['add_44[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_22 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " add_45 (Add)                   (None, 36, 41)       0           ['sequential_22[0][0]',          \n",
            "                                                                  'add_44[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_46 (LayerN  (None, 36, 41)      82          ['add_45[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (Multi  (None, 36, 41)      6053        ['layer_normalization_46[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " add_46 (Add)                   (None, 36, 41)       0           ['multi_head_attention_23[0][0]',\n",
            "                                                                  'add_45[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_47 (LayerN  (None, 36, 41)      82          ['add_46[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_23 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " add_47 (Add)                   (None, 36, 41)       0           ['sequential_23[0][0]',          \n",
            "                                                                  'add_46[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_48 (LayerN  (None, 36, 41)      82          ['add_47[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_24 (Multi  (None, 36, 41)      6053        ['layer_normalization_48[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " add_48 (Add)                   (None, 36, 41)       0           ['multi_head_attention_24[0][0]',\n",
            "                                                                  'add_47[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_49 (LayerN  (None, 36, 41)      82          ['add_48[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_24 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " add_49 (Add)                   (None, 36, 41)       0           ['sequential_24[0][0]',          \n",
            "                                                                  'add_48[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_50 (LayerN  (None, 36, 41)      82          ['add_49[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_25 (Multi  (None, 36, 41)      6053        ['layer_normalization_50[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " add_50 (Add)                   (None, 36, 41)       0           ['multi_head_attention_25[0][0]',\n",
            "                                                                  'add_49[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_51 (LayerN  (None, 36, 41)      82          ['add_50[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_25 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 36, 41)       0           ['sequential_25[0][0]',          \n",
            "                                                                  'add_50[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_52 (LayerN  (None, 36, 41)      82          ['add_51[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_26 (Multi  (None, 36, 41)      6053        ['layer_normalization_52[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 36, 41)       0           ['multi_head_attention_26[0][0]',\n",
            "                                                                  'add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_53 (LayerN  (None, 36, 41)      82          ['add_52[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_26 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 36, 41)       0           ['sequential_26[0][0]',          \n",
            "                                                                  'add_52[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_54 (LayerN  (None, 36, 41)      82          ['add_53[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 36, 41)      6053        ['layer_normalization_54[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 36, 41)       0           ['multi_head_attention_27[0][0]',\n",
            "                                                                  'add_53[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_55 (LayerN  (None, 36, 41)      82          ['add_54[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_27 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 36, 41)       0           ['sequential_27[0][0]',          \n",
            "                                                                  'add_54[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_56 (LayerN  (None, 36, 41)      82          ['add_55[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_28 (Multi  (None, 36, 41)      6053        ['layer_normalization_56[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 36, 41)       0           ['multi_head_attention_28[0][0]',\n",
            "                                                                  'add_55[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_57 (LayerN  (None, 36, 41)      82          ['add_56[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " sequential_28 (Sequential)     (None, 36, 41)       13653       ['layer_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 36, 41)       0           ['sequential_28[0][0]',          \n",
            "                                                                  'add_56[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_58 (LayerN  (None, 36, 41)      82          ['add_57[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 41)          0           ['layer_normalization_58[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 2)            84          ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 790,457\n",
            "Trainable params: 790,457\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/ucf_pretrained_on_lad_normal/checkpoint/'\n",
        "model_checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )"
      ],
      "metadata": {
        "id": "YtRHscscND2J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "checkpoint_filepath = '/content/gdrive/MyDrive/Checkpoints/ucf_pretrained_on_lad_normal/checkpoint/'\n",
        "checkpoint = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath=checkpoint_filepath,\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True\n",
        "                                )\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "                     monitor=\"val_loss\",\n",
        "                     min_delta=0.025,\n",
        "                     patience=5,\n",
        "                     verbose=0,\n",
        "                     mode=\"min\",\n",
        "                     baseline=None,\n",
        "                     restore_best_weights=False\n",
        "                 )\n",
        "\n",
        "values = np.linspace(0.00001,LEARNING_RATE,14)[::-1].astype(np.float32)\n",
        "boundaries = np.linspace(5, 45,13)[:values.shape[0]-1].astype(np.int32)\n",
        "\n",
        "scheduler = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    list(boundaries), list(values))\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=3, min_lr=0.000001)\n",
        "\n",
        "callbacks = [checkpoint, lr_scheduler, reduce_lr]"
      ],
      "metadata": {
        "id": "K4C-FTdXNWCg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment():\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    new_model = model\n",
        "    opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    new_model.compile(loss=loss_fn,optimizer=opt,metrics=['accuracy'])\n",
        "    #model.load_weights(checkpoint_filepath)\n",
        "    # Train the model.\n",
        "    history = new_model.fit(training_generator, epochs=10, validation_data=(validation_generator), callbacks=[model_checkpoint, callbacks])\n",
        "\n",
        "    return new_model, history\n",
        "\n",
        "\n",
        "model, history = run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8LljGphMLaw",
        "outputId": "2f546584-b7c9-4fd5-c7a4-ddb4098b2486"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/10\n",
            "1143/1143 [==============================] - 325s 172ms/step - loss: 0.6696 - accuracy: 0.5978 - val_loss: 0.5893 - val_accuracy: 0.8267 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 2/10\n",
            "1143/1143 [==============================] - 185s 162ms/step - loss: 0.6090 - accuracy: 0.6940 - val_loss: 0.5758 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 3/10\n",
            "1143/1143 [==============================] - 179s 157ms/step - loss: 0.5684 - accuracy: 0.7203 - val_loss: 0.6024 - val_accuracy: 0.7331 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 4/10\n",
            "1143/1143 [==============================] - 181s 159ms/step - loss: 0.5483 - accuracy: 0.7351 - val_loss: 0.6280 - val_accuracy: 0.6196 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 5/10\n",
            "1143/1143 [==============================] - 209s 183ms/step - loss: 0.5115 - accuracy: 0.7598 - val_loss: 0.6822 - val_accuracy: 0.5475 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 6/10\n",
            "1143/1143 [==============================] - 180s 158ms/step - loss: 0.4567 - accuracy: 0.7957 - val_loss: 0.6266 - val_accuracy: 0.6426 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 7/10\n",
            "1143/1143 [==============================] - 181s 158ms/step - loss: 0.3999 - accuracy: 0.8250 - val_loss: 0.6935 - val_accuracy: 0.7331 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 8/10\n",
            "1143/1143 [==============================] - 179s 157ms/step - loss: 0.3964 - accuracy: 0.8276 - val_loss: 1.1148 - val_accuracy: 0.5905 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 9.307692380389199e-05.\n",
            "Epoch 9/10\n",
            "1143/1143 [==============================] - 184s 161ms/step - loss: 0.3845 - accuracy: 0.8478 - val_loss: 1.0429 - val_accuracy: 0.5567 - lr: 9.3077e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 8.615384285803884e-05.\n",
            "Epoch 10/10\n",
            "1143/1143 [==============================] - 180s 158ms/step - loss: 0.2688 - accuracy: 0.9066 - val_loss: 1.0854 - val_accuracy: 0.5322 - lr: 8.6154e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "voSmkewE1ZcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "def eval_model(model,x,y):\n",
        "#     print(model.evaluate(x.reshape(x.shape+(1,)),y_encoded))\n",
        "    y_pred = model.predict(x)\n",
        "    y_pred = np.argmax(y_pred,axis = -1)\n",
        "    y_numbers = np.argmax(y,axis=-1)\n",
        "\n",
        "    target_names = ['normal','violence']\n",
        "    labels = target_names\n",
        "    tick_marks = np.arange(len(labels))\n",
        "\n",
        "    print(classification_report(y_numbers, y_pred, target_names=target_names))\n",
        "\n",
        "    z = tf.math.confusion_matrix(y_numbers,y_pred).numpy().astype(np.int64)\n",
        "    z = np.around(z.astype('float') / z.sum(axis=1)[:, np.newaxis], decimals=3)\n",
        "\n",
        "    x = target_names\n",
        "    y = target_names\n",
        "\n",
        "    # change each element of z to type string for annotations\n",
        "    z_text = [[str(y) for y in x] for x in z]\n",
        "\n",
        "    # set up figure\n",
        "    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
        "\n",
        "    # add title\n",
        "    fig.update_layout(title_text='<i><b>Confusion matrix</b></i>',\n",
        "                    #xaxis = dict(title='x'),\n",
        "                    #yaxis = dict(title='x')\n",
        "                    )\n",
        "\n",
        "    # add custom xaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=0.5,\n",
        "                            y=-0.15,\n",
        "                            showarrow=False,\n",
        "                            text=\"Predicted value\",\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # add custom yaxis title\n",
        "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                            x=-0.35,\n",
        "                            y=0.5,\n",
        "                            showarrow=False,\n",
        "                            text=\"Real value\",\n",
        "                            textangle=-90,\n",
        "                            xref=\"paper\",\n",
        "                            yref=\"paper\"))\n",
        "\n",
        "    # adjust margins to make room for yaxis title\n",
        "    fig.update_layout(margin=dict(t=50, l=200))\n",
        "\n",
        "    # add colorbar\n",
        "    fig['data'][0]['showscale'] = True\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "4SJywpkb7PC0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note: you may replicate the next cell to test on the train, validation or test dataset"
      ],
      "metadata": {
        "id": "fzPT2hT62LIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing on LAd2000"
      ],
      "metadata": {
        "id": "EKktG9Yj578M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf = np.moveaxis(X_test_ucf, 3, 1)"
      ],
      "metadata": {
        "id": "EGWUo2NzHtde"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_ucf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "791TC3z3H0hi",
        "outputId": "950bff75-c243-47bb-fbf4-aae7e345a78e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1345, 15, 75, 75, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, X_test_ucf, y_test_ucf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "ylEBMmz07XO2",
        "outputId": "7d81827e-66bc-4ed7-dcc4-7d01df7e8e79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 7s 53ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.00      0.00      0.00       682\n",
            "    violence       0.48      0.97      0.64       663\n",
            "\n",
            "    accuracy                           0.48      1345\n",
            "   macro avg       0.24      0.48      0.32      1345\n",
            "weighted avg       0.24      0.48      0.32      1345\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"7ddfc8c5-a765-4524-9c0d-c81d2818c9d9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7ddfc8c5-a765-4524-9c0d-c81d2818c9d9\")) {                    Plotly.newPlot(                        \"7ddfc8c5-a765-4524-9c0d-c81d2818c9d9\",                        [{\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"reversescale\":false,\"showscale\":true,\"x\":[\"normal\",\"violence\"],\"y\":[\"normal\",\"violence\"],\"z\":[[0.0,1.0],[0.035,0.965]],\"type\":\"heatmap\"}],                        {\"annotations\":[{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.0\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"1.0\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"normal\",\"yref\":\"y\"},{\"font\":{\"color\":\"#FFFFFF\"},\"showarrow\":false,\"text\":\"0.035\",\"x\":\"normal\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"#000000\"},\"showarrow\":false,\"text\":\"0.965\",\"x\":\"violence\",\"xref\":\"x\",\"y\":\"violence\",\"yref\":\"y\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Predicted value\",\"x\":0.5,\"xref\":\"paper\",\"y\":-0.15,\"yref\":\"paper\"},{\"font\":{\"color\":\"black\",\"size\":14},\"showarrow\":false,\"text\":\"Real value\",\"textangle\":-90,\"x\":-0.35,\"xref\":\"paper\",\"y\":0.5,\"yref\":\"paper\"}],\"xaxis\":{\"dtick\":1,\"gridcolor\":\"rgb(0, 0, 0)\",\"side\":\"top\",\"ticks\":\"\"},\"yaxis\":{\"dtick\":1,\"ticks\":\"\",\"ticksuffix\":\"  \"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"<i><b>Confusion matrix</b></i>\"},\"margin\":{\"t\":50,\"l\":200}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7ddfc8c5-a765-4524-9c0d-c81d2818c9d9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_ucf, y_test_ucf)"
      ],
      "metadata": {
        "id": "r3Ed-OUE2Frw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b77729-29ea-4d59-e580-a2b0b892dab9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 3s 65ms/step - loss: 1.1081 - accuracy: 0.5584\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1080777645111084, 0.5583643317222595]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix as cm\n",
        "\n",
        "def eval_model_f(model,x,y):\n",
        "#     print(model.evaluate(x.reshape(x.shape+(1,)),y_encoded))\n",
        "    y_pred = model.predict(x)\n",
        "    y_pred = np.argmax(y_pred,axis = -1)\n",
        "    y_numbers = np.argmax(y,axis=-1)\n",
        "\n",
        "    target_names = [\n",
        "      'normal',\n",
        "      'violence'\n",
        "    ]\n",
        "    tick_marks = np.arange(len(target_names))\n",
        "    print(classification_report(y_numbers, y_pred, target_names=target_names))\n",
        "\n",
        "    conf = cm(y_numbers,y_pred)\n",
        "\n",
        "    sns.heatmap(conf,annot=True)\n",
        "    plt.xticks(tick_marks,target_names,rotation=45)\n",
        "    plt.yticks(tick_marks,target_names,rotation=0)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yh7SM0s4IlGt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model_f(model, X_test_ucf, y_test_ucf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "zOWXO7S9Iogh",
        "outputId": "1f86408b-98f2-4bd7-b8de-8f72544b90e2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 5s 108ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      normal       0.00      0.00      0.00       682\n",
            "    violence       0.48      0.97      0.64       663\n",
            "\n",
            "    accuracy                           0.48      1345\n",
            "   macro avg       0.24      0.48      0.32      1345\n",
            "weighted avg       0.24      0.48      0.32      1345\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z00_j8e_IseD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}